{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mo1_LSGDWjo"
      },
      "outputs": [],
      "source": [
        "# Create necessary directories\n",
        "!mkdir -p modules\n",
        "!mkdir -p data\n",
        "!mkdir -p assets"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y2kq_j-Mkaon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install torch tqdm numpy pandas matplotlib networkx igraph statsmodels dython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsObNbjAEVhr",
        "outputId": "537e2fcb-107e-4e50-a043-6c7cea056f16",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (3.4.2)\n",
            "Collecting igraph\n",
            "  Downloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (0.14.4)\n",
            "Collecting dython\n",
            "  Downloading dython-0.7.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Collecting texttable>=1.6.2 (from igraph)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.14.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from dython) (0.13.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.11/dist-packages (from dython) (1.6.1)\n",
            "Requirement already satisfied: psutil>=5.9.1 in /usr/local/lib/python3.11/dist-packages (from dython) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dython) (75.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.2->dython) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.2->dython) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dython-0.7.9-py3-none-any.whl (26 kB)\n",
            "Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, igraph, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, dython\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed dython-0.7.9 igraph-0.11.8 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 texttable-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ADULT DATASET**"
      ],
      "metadata": {
        "id": "zW1v4nKfNJcD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and prepare the Adult dataset\n",
        "!mkdir -p data\n",
        "\n",
        "# Download the Adult dataset\n",
        "!wget -O data/adult.data https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
        "!wget -O data/adult.test https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
        "\n",
        "# Process the data\n",
        "import pandas as pd\n",
        "\n",
        "# Define column names\n",
        "columns = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'educational-num',\n",
        "    'marital-status', 'occupation', 'relationship', 'race', 'gender',\n",
        "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
        "]\n",
        "\n",
        "# Read train data\n",
        "train_data = pd.read_csv('data/adult.data', header=None, names=columns, sep=', ', engine='python')\n",
        "\n",
        "# Read test data (skip the first line as it's a header)\n",
        "test_data = pd.read_csv('data/adult.test', header=None, names=columns, sep=', ', engine='python', skiprows=1)\n",
        "test_data['income'] = test_data['income'].str.replace('.', '')  # Remove period at the end of income values\n",
        "\n",
        "# Combine and save\n",
        "combined_data = pd.concat([train_data, test_data])\n",
        "combined_data.to_csv('data/adult.csv', index=False)\n",
        "\n",
        "print(\"Adult dataset preparation complete!\")"
      ],
      "metadata": {
        "id": "Ue0RiUk_FwMC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "ba92df44-7812-45f3-8007-28dab8ffca13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-24 14:56:52--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘data/adult.data’\n",
            "\n",
            "data/adult.data         [     <=>            ]   3.79M  3.25MB/s    in 1.2s    \n",
            "\n",
            "2025-04-24 14:56:54 (3.25 MB/s) - ‘data/adult.data’ saved [3974305]\n",
            "\n",
            "--2025-04-24 14:56:54--  https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘data/adult.test’\n",
            "\n",
            "data/adult.test         [    <=>             ]   1.91M  1.85MB/s    in 1.0s    \n",
            "\n",
            "2025-04-24 14:56:56 (1.85 MB/s) - ‘data/adult.test’ saved [2003153]\n",
            "\n",
            "Adult dataset preparation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Forest Cover Type Dataset**"
      ],
      "metadata": {
        "id": "ipDWz5ixOQDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download and prepare the Forest Cover Type dataset\n",
        "!mkdir -p data\n",
        "\n",
        "# Download covtype dataset\n",
        "!wget -O data/covtype.data.gz https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
        "!gunzip data/covtype.data.gz\n",
        "\n",
        "# Process the file\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Define column names\n",
        "columns = [\n",
        "    'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
        "    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
        "    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
        "    'Horizontal_Distance_To_Fire_Points'\n",
        "]\n",
        "\n",
        "# Add soil type and wilderness area columns\n",
        "for i in range(4):\n",
        "    columns.append(f'Wilderness_Area_{i+1}')\n",
        "for i in range(40):\n",
        "    columns.append(f'Soil_Type_{i+1}')\n",
        "\n",
        "columns.append('Cover_Type')\n",
        "\n",
        "# Read the data\n",
        "data = pd.read_csv('data/covtype.data', header=None, names=columns)\n",
        "\n",
        "# Process data (simplified for this dataset)\n",
        "covtype_data = data[['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
        "                     'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
        "                     'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
        "                     'Horizontal_Distance_To_Fire_Points', 'Cover_Type']]\n",
        "\n",
        "# Save to CSV\n",
        "covtype_data.to_csv('data/covtype.csv', index=False)\n",
        "\n",
        "print(\"Cover type dataset preparation complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fArJo7u0Nane",
        "outputId": "de877c17-e232-4bc4-a1ad-fc976aa54f22",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-24 14:57:25--  https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘data/covtype.data.gz’\n",
            "\n",
            "data/covtype.data.g     [      <=>           ]  10.72M  6.73MB/s    in 1.6s    \n",
            "\n",
            "2025-04-24 14:57:28 (6.73 MB/s) - ‘data/covtype.data.gz’ saved [11240707]\n",
            "\n",
            "Cover type dataset preparation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Home Credit Default Risk Dataset**"
      ],
      "metadata": {
        "id": "Tp9D-mKgVVxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the Home Credit Default Risk Dataset\n",
        "!mkdir -p data\n",
        "\n",
        "# Unzip the application_train.csv.zip file\n",
        "!unzip -o /content/data/application_train.csv.zip -d /content/data/\n",
        "\n",
        "# Process the data\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/data/application_train.csv')\n",
        "\n",
        "# Take a sample if needed to reduce size (optional, remove if you want to use all data)\n",
        "df = df.sample(n=50000, random_state=0) if len(df) > 50000 else df\n",
        "\n",
        "# Save to CSV at the expected path\n",
        "df.to_csv('/content/data/application_train.csv', index=False)\n",
        "\n",
        "print(\"Home Credit Default Risk dataset preparation complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv-yTb7HOUYV",
        "outputId": "5dd8e957-80c2-4a43-d358-fe1c70040a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data/application_train.csv.zip\n",
            "  inflating: /content/data/application_train.csv  \n",
            "Home Credit Default Risk dataset preparation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PERSONAL LOAN MODELLING DATASET**"
      ],
      "metadata": {
        "id": "knDKSNYzTF-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the Personal Loan Modeling Dataset\n",
        "!mkdir -p data\n",
        "\n",
        "# Process the data\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/data/Bank_Personal_Loan_Modelling.csv')\n",
        "\n",
        "# No need to sample as this dataset is small\n",
        "\n",
        "# Save to CSV at the expected path\n",
        "df.to_csv('/content/data/Bank_Personal_Loan_Modelling.csv', index=False)\n",
        "\n",
        "print(\"Personal Loan Modeling dataset preparation complete!\")"
      ],
      "metadata": {
        "id": "W04QfsjINXhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "95f21e06-a8e5-435b-f703-bb0a15bd382e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Personal Loan Modeling dataset preparation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Taxi Pricing (Cabs)**"
      ],
      "metadata": {
        "id": "CIgIk76xW3q6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the Taxi Pricing (Cabs) Dataset\n",
        "!mkdir -p data\n",
        "\n",
        "# Unzip the test.csv.zip file\n",
        "!unzip -o /content/data/test.csv.zip -d /content/data/\n",
        "\n",
        "# Process the data\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/data/test.csv')\n",
        "\n",
        "# Rename the file to what the code expects\n",
        "df.to_csv('/content/data/sigma_cabs.csv', index=False)\n",
        "\n",
        "print(\"Taxi Pricing (Cabs) dataset preparation complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "x5v8UYXFVQEW",
        "outputId": "e3a33d08-3b48-4527-d48b-cbf76d7dc65b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data/test.csv.zip\n",
            "  inflating: /content/data/test.csv  \n",
            "Taxi Pricing (Cabs) dataset preparation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **King County House Sales Dataset**"
      ],
      "metadata": {
        "id": "akRDuKg9XNZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the King County House Sales Dataset\n",
        "!mkdir -p data\n",
        "\n",
        "# Unzip the kc_house_data.csv.zip file\n",
        "!unzip -o /content/data/kc_house_data.csv.zip -d /content/data/\n",
        "\n",
        "# Process the data\n",
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('/content/data/kc_house_data.csv')\n",
        "\n",
        "# No need to rename this file as it already has the expected name\n",
        "# Just saving it back to ensure it's properly formatted\n",
        "df.to_csv('/content/data/kc_house_data.csv', index=False)\n",
        "\n",
        "print(\"King County House Sales dataset preparation complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A8d-vve2XMng",
        "outputId": "c0f21634-255d-47e7-e9d8-f971dcd8bb6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/data/kc_house_data.csv.zip\n",
            "  inflating: /content/data/kc_house_data.csv  \n",
            "King County House Sales dataset preparation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('modules/simulation.py', 'w') as f:\n",
        "    f.write('''\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\"\"\"for reproducibility\"\"\"\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    # torch.cuda.manual_seed_all(seed) # if use multi-GPU\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "''')\n",
        "\n",
        "print(\"Created simulation.py in the modules folder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZJedz-_bnWb",
        "outputId": "9146a20e-72f8-4cd6-852d-f4f639660aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created simulation.py in the modules folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation of DistVAE for Synthetic Tabular Data Generation**"
      ],
      "metadata": {
        "id": "UxG5yn-Ya170"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('modules/model.py', 'w') as f:\n",
        "    f.write('''import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, config, device):\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        \"\"\"encoder\"\"\"\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(config[\"CRPS_dim\"] + config[\"softmax_dim\"], 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(8, config[\"latent_dim\"] * 2),\n",
        "        ).to(device)\n",
        "\n",
        "        \"\"\"spline\"\"\"\n",
        "        self.delta = torch.arange(0, 1 + config[\"step\"], step=config[\"step\"]).view(1, -1).to(device)\n",
        "        self.M = self.delta.size(1) - 1\n",
        "        self.spline = nn.Sequential(\n",
        "            nn.Linear(config[\"latent_dim\"], 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, config[\"CRPS_dim\"] * (1 + (self.M + 1)) + config[\"softmax_dim\"]),\n",
        "        ).to(device)\n",
        "\n",
        "    def get_posterior(self, input):\n",
        "        h = self.encoder(input)\n",
        "        mean, logvar = torch.split(h, self.config[\"latent_dim\"], dim=1)\n",
        "        return mean, logvar\n",
        "\n",
        "    def sampling(self, mean, logvar, deterministic=False):\n",
        "        if deterministic:\n",
        "            z = mean\n",
        "        else:\n",
        "            noise = torch.randn(mean.size(0), self.config[\"latent_dim\"]).to(self.device)\n",
        "            z = mean + torch.exp(logvar / 2) * noise\n",
        "        return z\n",
        "\n",
        "    def encode(self, input, deterministic=False):\n",
        "        mean, logvar = self.get_posterior(input)\n",
        "        z = self.sampling(mean, logvar, deterministic=deterministic)\n",
        "        return z, mean, logvar\n",
        "\n",
        "    def quantile_parameter(self, z):\n",
        "        # Make sure z is on the correct device\n",
        "        z = z.to(self.device)\n",
        "        h = self.spline(z)\n",
        "        logit = h[:, -self.config[\"softmax_dim\"]:]\n",
        "        spline = h[:, :-self.config[\"softmax_dim\"]]\n",
        "        h = torch.split(spline, 1 + (self.M + 1), dim=1)\n",
        "\n",
        "        gamma = [h_[:, [0]] for h_ in h]\n",
        "        beta = [nn.Softplus()(h_[:, 1:]) for h_ in h] # positive constraint\n",
        "        return gamma, beta, logit\n",
        "\n",
        "    def quantile_function(self, alpha, gamma, beta, j):\n",
        "        return gamma[j] + (beta[j] * torch.where(alpha - self.delta > 0,\n",
        "                                                alpha - self.delta,\n",
        "                                                torch.zeros(()).to(self.device))).sum(axis=1, keepdims=True)\n",
        "\n",
        "    def _quantile_inverse(self, x, gamma, beta, j):\n",
        "        delta_ = self.delta.unsqueeze(2).repeat(1, 1, self.M + 1)\n",
        "        delta_ = torch.where(delta_ - self.delta > 0,\n",
        "                            delta_ - self.delta,\n",
        "                            torch.zeros(()).to(self.device))\n",
        "        mask = gamma[j] + (beta[j] * delta_.unsqueeze(2)).sum(axis=-1).squeeze(0).t()\n",
        "        mask = torch.where(mask <= x,\n",
        "                        mask,\n",
        "                        torch.zeros(()).to(self.device)).type(torch.bool).type(torch.float)\n",
        "        alpha_tilde = x - gamma[j]\n",
        "        alpha_tilde += (mask * beta[j] * self.delta).sum(axis=1, keepdims=True)\n",
        "        alpha_tilde /= (mask * beta[j]).sum(axis=1, keepdims=True) + 1e-6\n",
        "        alpha_tilde = torch.clip(alpha_tilde, self.config[\"threshold\"], 1) # numerical stability\n",
        "        return alpha_tilde\n",
        "\n",
        "    def quantile_inverse(self, x, gamma, beta):\n",
        "        alpha_tilde_list = []\n",
        "        for j in range(self.config[\"CRPS_dim\"]):\n",
        "            alpha_tilde = self._quantile_inverse(x[:, [j]], gamma, beta, j)\n",
        "            alpha_tilde_list.append(alpha_tilde)\n",
        "        return alpha_tilde_list\n",
        "\n",
        "    def forward(self, input, deterministic=False):\n",
        "        z, mean, logvar = self.encode(input, deterministic=deterministic)\n",
        "        gamma, beta, logit = self.quantile_parameter(z)\n",
        "        return z, mean, logvar, gamma, beta, logit\n",
        "\n",
        "    def gumbel_sampling(self, size, eps = 1e-20):\n",
        "        U = torch.rand(size).to(self.device)\n",
        "        G = (- (U + eps).log() + eps).log()\n",
        "        return G\n",
        "\n",
        "    def generate_data(self, n, OutputInfo_list, dataset, reverse_col=False):\n",
        "        data = []\n",
        "        steps = n // self.config[\"batch_size\"] + 1\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(steps):\n",
        "                # Use device for randn\n",
        "                randn = torch.randn(self.config[\"batch_size\"], self.config[\"latent_dim\"], device=self.device)\n",
        "                gamma, beta, logit = self.quantile_parameter(randn)\n",
        "\n",
        "                samples = []\n",
        "                st = 0\n",
        "                for j, info in enumerate(OutputInfo_list):\n",
        "                    if info.activation_fn == \"CRPS\":\n",
        "                        # Use device for alpha\n",
        "                        alpha = torch.rand(self.config[\"batch_size\"], 1, device=self.device)\n",
        "                        samples.append(self.quantile_function(alpha, gamma, beta, j))\n",
        "\n",
        "                    elif info.activation_fn == \"softmax\":\n",
        "                        ed = st + info.dim\n",
        "                        out = logit[:, st : ed]\n",
        "\n",
        "                        \"\"\"Gumbel-Max Trick\"\"\"\n",
        "                        G = self.gumbel_sampling(out.shape)\n",
        "                        _, out = (nn.LogSoftmax(dim=1)(out) + G).max(dim=1)\n",
        "\n",
        "                        samples.append(out.unsqueeze(1))\n",
        "                        # samples.append(F.one_hot(out, num_classes=info.dim))\n",
        "                        st = ed\n",
        "\n",
        "                samples = torch.cat(samples, dim=1)\n",
        "                data.append(samples)\n",
        "        data = torch.cat(data, dim=0)\n",
        "        data = data[:n, :]\n",
        "        # Move data to CPU for pandas processing\n",
        "        data = data.cpu().numpy()\n",
        "        data = pd.DataFrame(data, columns=dataset.continuous + dataset.discrete)\n",
        "\n",
        "        \"\"\"un-standardization of synthetic data\"\"\"\n",
        "        data[dataset.continuous] = data[dataset.continuous] * dataset.std.to_numpy() + dataset.mean.to_numpy()\n",
        "\n",
        "        \"\"\"post-process integer columns (calibration)\"\"\"\n",
        "        data[dataset.integer] = data[dataset.integer].round(0).astype(int)\n",
        "        data[dataset.discrete] = data[dataset.discrete].astype(int)\n",
        "\n",
        "        if reverse_col:\n",
        "            \"\"\"reverse to original column names\"\"\"\n",
        "            for dis, disdict in zip(dataset.discrete, dataset.discrete_dicts_reverse):\n",
        "                data[dis] = data[dis].apply(lambda x: disdict.get(x))\n",
        "\n",
        "        return data\n",
        "''')\n",
        "\n",
        "print(\"Created model.py in the modules folder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdY5a8tlc6dt",
        "outputId": "65c42349-a96a-4fc4-d66c-7ca6ff227ded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created model.py in the modules folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('modules/train.py', 'w') as f:\n",
        "    f.write('''\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def train_VAE(OutputInfo_list, dataloader, model, config, optimizer, device):\n",
        "    logs = {\n",
        "        'loss': [],\n",
        "        'quantile': [],\n",
        "        'KL': [],\n",
        "    }\n",
        "    # for debugging\n",
        "    logs['activated'] = []\n",
        "\n",
        "    for (x_batch) in tqdm.tqdm(iter(dataloader), desc=\"inner loop\"):\n",
        "\n",
        "        if config[\"cuda\"]:\n",
        "            x_batch = x_batch.cuda()\n",
        "\n",
        "        # with torch.autograd.set_detect_anomaly(True):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        z, mean, logvar, gamma, beta, logit = model(x_batch)\n",
        "\n",
        "        loss_ = []\n",
        "\n",
        "        \"\"\"alpha_tilde\"\"\"\n",
        "        alpha_tilde_list = model.quantile_inverse(x_batch, gamma, beta)\n",
        "\n",
        "        \"\"\"loss\"\"\"\n",
        "        j = 0\n",
        "        st = 0\n",
        "        total_loss = 0\n",
        "        # tmp1 = []\n",
        "        # tmp2 = []\n",
        "        for j, info in enumerate(OutputInfo_list):\n",
        "            if info.activation_fn == \"CRPS\":\n",
        "                term = (1 - model.delta.pow(3)) / 3 - model.delta - torch.maximum(alpha_tilde_list[j], model.delta).pow(2)\n",
        "                term += 2 * torch.maximum(alpha_tilde_list[j], model.delta) * model.delta\n",
        "\n",
        "                loss = (2 * alpha_tilde_list[j]) * x_batch[:, [j]]\n",
        "                loss += (1 - 2 * alpha_tilde_list[j]) * gamma[j]\n",
        "                loss += (beta[j] * term).sum(axis=1, keepdims=True)\n",
        "                loss *= 0.5\n",
        "                total_loss += loss.mean()\n",
        "                # tmp1.append(x_batch[:, [j]])\n",
        "\n",
        "            elif info.activation_fn == \"softmax\":\n",
        "                ed = st + info.dim\n",
        "                _, targets = x_batch[:, config[\"CRPS_dim\"] + st : config[\"CRPS_dim\"] + ed].max(dim=1)\n",
        "                out = logit[:, st : ed]\n",
        "                # tmp1.append(x_batch[:, config[\"CRPS_dim\"] + st : config[\"CRPS_dim\"] + ed])\n",
        "                # tmp2.append(out)\n",
        "                total_loss += nn.CrossEntropyLoss()(out, targets)\n",
        "                st = ed\n",
        "\n",
        "        # assert (torch.cat(tmp1, dim=1) - x_batch).sum().item() == 0\n",
        "        # assert (torch.cat(tmp2, dim=1) - logit).sum().item() == 0\n",
        "\n",
        "        loss_.append(('quantile', total_loss))\n",
        "\n",
        "        \"\"\"KL-Divergence\"\"\"\n",
        "        KL = torch.pow(mean, 2).sum(axis=1)\n",
        "        KL -= logvar.sum(axis=1)\n",
        "        KL += torch.exp(logvar).sum(axis=1)\n",
        "        KL -= config[\"latent_dim\"]\n",
        "        KL *= 0.5\n",
        "        KL = KL.mean()\n",
        "        loss_.append(('KL', KL))\n",
        "\n",
        "        ### activated: for debugging\n",
        "        var_ = torch.exp(logvar).mean(axis=0)\n",
        "        loss_.append(('activated', (var_ < 0.1).sum()))\n",
        "\n",
        "        loss = total_loss + config[\"beta\"] * KL\n",
        "        loss_.append(('loss', loss))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        \"\"\"accumulate losses\"\"\"\n",
        "        for x, y in loss_:\n",
        "            logs[x] = logs.get(x) + [y.item()]\n",
        "\n",
        "    return logs\n",
        "''')\n",
        "\n",
        "print(\"Created train.py in the modules folder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdPzqZU8auOf",
        "outputId": "2c21450c-e2b0-4c0a-ce3d-1deff85eae0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created train.py in the modules folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the data type issues in all dataset modules\n",
        "import os\n",
        "\n",
        "for dataset in ['adult', 'covtype', 'credit', 'loan', 'cabs', 'kings']:\n",
        "    dataset_path = f'modules/{dataset}_datasets.py'\n",
        "\n",
        "    # Check if file exists\n",
        "    if os.path.exists(dataset_path):\n",
        "        with open(dataset_path, 'r') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        # Modify content to use float32\n",
        "        modified_content = content.replace(\n",
        "            \"self.x_data = df.to_numpy()\",\n",
        "            \"self.x_data = df.to_numpy().astype(np.float32)\"\n",
        "        )\n",
        "\n",
        "        # Write back\n",
        "        with open(dataset_path, 'w') as f:\n",
        "            f.write(modified_content)\n",
        "\n",
        "        print(f\"Fixed {dataset}_datasets.py to handle data type conversion\")"
      ],
      "metadata": {
        "id": "1af1jQBdbBm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('modules/adult_datasets.py', 'w') as f:\n",
        "    f.write('''\n",
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "OutputInfo = namedtuple('OutputInfo', ['dim', 'activation_fn'])\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        base = pd.read_csv('./data/adult.csv')\n",
        "        base = base.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "        base = base[(base == '?').sum(axis=1) == 0]\n",
        "\n",
        "        self.continuous = [\n",
        "            'age', # target variable\n",
        "            'educational-num',\n",
        "            'capital-gain',\n",
        "            'capital-loss',\n",
        "            'hours-per-week',\n",
        "        ]\n",
        "        self.discrete = [\n",
        "            'workclass',\n",
        "            'education',\n",
        "            'marital-status',\n",
        "            'occupation',\n",
        "            'relationship',\n",
        "            'race',\n",
        "            'gender',\n",
        "            'native-country',\n",
        "            'income', # target variable\n",
        "        ]\n",
        "        self.integer = self.continuous\n",
        "        base = base[self.continuous + self.discrete]\n",
        "        base = base.dropna()\n",
        "\n",
        "        self.discrete_dicts = []\n",
        "        self.discrete_dicts_reverse = []\n",
        "        for dis in self.discrete:\n",
        "            discrete_dict = {x:i for i,x in enumerate(sorted(base[dis].unique()))}\n",
        "            self.discrete_dicts_reverse.append({i:x for i,x in enumerate(sorted(base[dis].unique()))})\n",
        "            base[dis] = base[dis].apply(lambda x: discrete_dict.get(x))\n",
        "            self.discrete_dicts.append(discrete_dict)\n",
        "\n",
        "        self.RegTarget = 'age'\n",
        "        self.ClfTarget = 'income'\n",
        "\n",
        "        # one-hot encoding\n",
        "        df_dummy = []\n",
        "        for d in self.discrete:\n",
        "            df_dummy.append(pd.get_dummies(base[d], prefix=d))\n",
        "        base_dummy = pd.concat([base.drop(columns=self.discrete)] + df_dummy, axis=1)\n",
        "\n",
        "        split_num = 40000\n",
        "\n",
        "        if train:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "\n",
        "            df = base_dummy.iloc[:split_num] # train\n",
        "\n",
        "            self.mean = df[self.continuous].mean(axis=0)\n",
        "            self.std = df[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.train = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)  # Ensure float32 type\n",
        "        else:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "            self.test_raw = base.iloc[split_num:]\n",
        "\n",
        "            df_train = base_dummy.iloc[:split_num] # train\n",
        "            df = base_dummy.iloc[split_num:] # test\n",
        "\n",
        "            self.mean = df_train[self.continuous].mean(axis=0)\n",
        "            self.std = df_train[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.test = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)  # Ensure float32 type\n",
        "\n",
        "        # Output Information\n",
        "        self.OutputInfo_list = []\n",
        "        for c in self.continuous:\n",
        "            self.OutputInfo_list.append(OutputInfo(1, 'CRPS'))\n",
        "        for d, dummy in zip(self.discrete, df_dummy):\n",
        "            self.OutputInfo_list.append(OutputInfo(dummy.shape[1], 'softmax'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.FloatTensor(self.x_data[idx])\n",
        "        return x\n",
        "''')\n",
        "\n",
        "print(\"Created adult_datasets.py in the modules folder\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWsggx45deKw",
        "outputId": "9032cfe5-40c9-4925-ce14-34b97d3b2a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created adult_datasets.py in the modules folder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create covtype_datasets.py\n",
        "with open('modules/covtype_datasets.py', 'w') as f:\n",
        "    f.write('''\n",
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "OutputInfo = namedtuple('OutputInfo', ['dim', 'activation_fn'])\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        base = pd.read_csv('./data/covtype.csv')\n",
        "        base = base.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "        base = base.dropna(axis=0)\n",
        "        base = base.iloc[:50000]\n",
        "\n",
        "        self.continuous = [\n",
        "            'Elevation', # target variable\n",
        "            'Aspect',\n",
        "            'Slope',\n",
        "            'Horizontal_Distance_To_Hydrology',\n",
        "            'Vertical_Distance_To_Hydrology',\n",
        "            'Horizontal_Distance_To_Roadways',\n",
        "            'Hillshade_9am',\n",
        "            'Hillshade_Noon',\n",
        "            'Hillshade_3pm',\n",
        "            'Horizontal_Distance_To_Fire_Points',\n",
        "        ]\n",
        "        self.discrete = [\n",
        "            'Cover_Type', # target variable\n",
        "        ]\n",
        "        self.integer = self.continuous\n",
        "        base = base[self.continuous + self.discrete]\n",
        "\n",
        "        self.discrete_dicts = []\n",
        "        self.discrete_dicts_reverse = []\n",
        "        for dis in self.discrete:\n",
        "            discrete_dict = {x:i for i,x in enumerate(sorted(base[dis].unique()))}\n",
        "            self.discrete_dicts_reverse.append({i:x for i,x in enumerate(sorted(base[dis].unique()))})\n",
        "            base[dis] = base[dis].apply(lambda x: discrete_dict.get(x))\n",
        "            self.discrete_dicts.append(discrete_dict)\n",
        "\n",
        "        self.RegTarget = 'Elevation'\n",
        "        self.ClfTarget = 'Cover_Type'\n",
        "\n",
        "        # one-hot encoding\n",
        "        df_dummy = []\n",
        "        for d in self.discrete:\n",
        "            df_dummy.append(pd.get_dummies(base[d], prefix=d))\n",
        "        base_dummy = pd.concat([base.drop(columns=self.discrete)] + df_dummy, axis=1)\n",
        "\n",
        "        split_num = 45000\n",
        "\n",
        "        if train:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "\n",
        "            df = base_dummy.iloc[:split_num] # train\n",
        "\n",
        "            self.mean = df[self.continuous].mean(axis=0)\n",
        "            self.std = df[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.train = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)\n",
        "        else:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "            self.test_raw = base.iloc[split_num:]\n",
        "\n",
        "            df_train = base_dummy.iloc[:split_num] # train\n",
        "            df = base_dummy.iloc[split_num:] # test\n",
        "\n",
        "            self.mean = df_train[self.continuous].mean(axis=0)\n",
        "            self.std = df_train[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.test = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)\n",
        "\n",
        "        # Output Information\n",
        "        self.OutputInfo_list = []\n",
        "        for c in self.continuous:\n",
        "            self.OutputInfo_list.append(OutputInfo(1, 'CRPS'))\n",
        "        for d, dummy in zip(self.discrete, df_dummy):\n",
        "            self.OutputInfo_list.append(OutputInfo(dummy.shape[1], 'softmax'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.FloatTensor(self.x_data[idx])\n",
        "        return x\n",
        "''')\n",
        "\n",
        "# Create credit_datasets.py\n",
        "with open('modules/credit_datasets.py', 'w') as f:\n",
        "    f.write('''\n",
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "OutputInfo = namedtuple('OutputInfo', ['dim', 'activation_fn'])\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        base = pd.read_csv('./data/application_train.csv')\n",
        "        base = base.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "        self.continuous = [\n",
        "            'AMT_INCOME_TOTAL',\n",
        "            'AMT_CREDIT', # target variable\n",
        "            'AMT_ANNUITY',\n",
        "            'AMT_GOODS_PRICE',\n",
        "            'REGION_POPULATION_RELATIVE',\n",
        "            'DAYS_BIRTH',\n",
        "            'DAYS_EMPLOYED',\n",
        "            'DAYS_REGISTRATION',\n",
        "            'DAYS_ID_PUBLISH',\n",
        "            'OWN_CAR_AGE',\n",
        "        ]\n",
        "        self.discrete = [\n",
        "            'NAME_CONTRACT_TYPE',\n",
        "            'CODE_GENDER',\n",
        "            'FLAG_OWN_REALTY',\n",
        "            'NAME_TYPE_SUITE',\n",
        "            'NAME_INCOME_TYPE',\n",
        "            'NAME_EDUCATION_TYPE',\n",
        "            'NAME_FAMILY_STATUS',\n",
        "            'NAME_HOUSING_TYPE',\n",
        "            'TARGET', # target variable\n",
        "        ]\n",
        "        self.integer = [\n",
        "            'DAYS_BIRTH',\n",
        "            'DAYS_EMPLOYED',\n",
        "            'DAYS_ID_PUBLISH']\n",
        "        base = base[self.continuous + self.discrete]\n",
        "        base = base.dropna()\n",
        "        base = base.iloc[:50000]\n",
        "\n",
        "        self.discrete_dicts = []\n",
        "        self.discrete_dicts_reverse = []\n",
        "        for dis in self.discrete:\n",
        "            discrete_dict = {x:i for i,x in enumerate(sorted(base[dis].unique()))}\n",
        "            self.discrete_dicts_reverse.append({i:x for i,x in enumerate(sorted(base[dis].unique()))})\n",
        "            base[dis] = base[dis].apply(lambda x: discrete_dict.get(x))\n",
        "            self.discrete_dicts.append(discrete_dict)\n",
        "\n",
        "        self.RegTarget = 'AMT_CREDIT'\n",
        "        self.ClfTarget = 'TARGET'\n",
        "\n",
        "        # one-hot encoding\n",
        "        df_dummy = []\n",
        "        for d in self.discrete:\n",
        "            df_dummy.append(pd.get_dummies(base[d], prefix=d))\n",
        "        base_dummy = pd.concat([base.drop(columns=self.discrete)] + df_dummy, axis=1)\n",
        "\n",
        "        split_num = 45000\n",
        "\n",
        "        if train:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "\n",
        "            df = base_dummy.iloc[:split_num] # train\n",
        "\n",
        "            self.mean = df[self.continuous].mean(axis=0)\n",
        "            self.std = df[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.train = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)\n",
        "        else:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "            self.test_raw = base.iloc[split_num:]\n",
        "\n",
        "            df_train = base_dummy.iloc[:split_num] # train\n",
        "            df = base_dummy.iloc[split_num:] # test\n",
        "\n",
        "            self.mean = df_train[self.continuous].mean(axis=0)\n",
        "            self.std = df_train[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.test = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)\n",
        "\n",
        "        # Output Information\n",
        "        self.OutputInfo_list = []\n",
        "        for c in self.continuous:\n",
        "            self.OutputInfo_list.append(OutputInfo(1, 'CRPS'))\n",
        "        for d, dummy in zip(self.discrete, df_dummy):\n",
        "            self.OutputInfo_list.append(OutputInfo(dummy.shape[1], 'softmax'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.FloatTensor(self.x_data[idx])\n",
        "        return x\n",
        "''')\n",
        "\n",
        "# Create loan_datasets.py\n",
        "with open('modules/loan_datasets.py', 'w') as f:\n",
        "    f.write('''\n",
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "OutputInfo = namedtuple('OutputInfo', ['dim', 'activation_fn'])\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        base = pd.read_csv('./data/Bank_Personal_Loan_Modelling.csv')\n",
        "        base = base.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "        self.continuous = [\n",
        "            'Age', # target variable\n",
        "            'Experience',\n",
        "            'Income',\n",
        "            'CCAvg',\n",
        "            'Mortgage',\n",
        "        ]\n",
        "        self.discrete = [\n",
        "            'Family',\n",
        "            'Personal Loan', # target variable\n",
        "            'Securities Account',\n",
        "            'CD Account',\n",
        "            'Online',\n",
        "            'CreditCard'\n",
        "        ]\n",
        "        self.integer = [\n",
        "            'Age',\n",
        "            'Experience',\n",
        "            'Income',\n",
        "            'Mortgage']\n",
        "        base = base[self.continuous + self.discrete]\n",
        "        base = base.dropna()\n",
        "\n",
        "        self.discrete_dicts = []\n",
        "        self.discrete_dicts_reverse = []\n",
        "        for dis in self.discrete:\n",
        "            discrete_dict = {x:i for i,x in enumerate(sorted(base[dis].unique()))}\n",
        "            self.discrete_dicts_reverse.append({i:x for i,x in enumerate(sorted(base[dis].unique()))})\n",
        "            base[dis] = base[dis].apply(lambda x: discrete_dict.get(x))\n",
        "            self.discrete_dicts.append(discrete_dict)\n",
        "\n",
        "        self.RegTarget = 'Age'\n",
        "        self.ClfTarget = 'Personal Loan'\n",
        "\n",
        "        # one-hot encoding\n",
        "        df_dummy = []\n",
        "        for d in self.discrete:\n",
        "            df_dummy.append(pd.get_dummies(base[d], prefix=d))\n",
        "        base_dummy = pd.concat([base.drop(columns=self.discrete)] + df_dummy, axis=1)\n",
        "\n",
        "        split_num = 4000\n",
        "\n",
        "        if train:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "\n",
        "            df = base_dummy.iloc[:split_num] # train\n",
        "\n",
        "            self.mean = df[self.continuous].mean(axis=0)\n",
        "            self.std = df[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.train = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)\n",
        "        else:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "            self.test_raw = base.iloc[split_num:]\n",
        "\n",
        "            df_train = base_dummy.iloc[:split_num] # train\n",
        "            df = base_dummy.iloc[split_num:] # test\n",
        "\n",
        "            self.mean = df_train[self.continuous].mean(axis=0)\n",
        "            self.std = df_train[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.test = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)\n",
        "\n",
        "        # Output Information\n",
        "        self.OutputInfo_list = []\n",
        "        for c in self.continuous:\n",
        "            self.OutputInfo_list.append(OutputInfo(1, 'CRPS'))\n",
        "        for d, dummy in zip(self.discrete, df_dummy):\n",
        "            self.OutputInfo_list.append(OutputInfo(dummy.shape[1], 'softmax'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.FloatTensor(self.x_data[idx])\n",
        "        return x\n",
        "''')\n",
        "\n",
        "\n",
        "# Create kings_datasets.py\n",
        "with open('modules/kings_datasets.py', 'w') as f:\n",
        "    f.write('''\n",
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "OutputInfo = namedtuple('OutputInfo', ['dim', 'activation_fn'])\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        base = pd.read_csv('./data/kc_house_data.csv')\n",
        "        base = base.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "        self.continuous = [\n",
        "            'price',\n",
        "            'sqft_living',\n",
        "            'sqft_lot',\n",
        "            'sqft_above',\n",
        "            'sqft_basement',\n",
        "            'yr_built',\n",
        "            'yr_renovated',\n",
        "            'lat',\n",
        "            'long', # target variable\n",
        "            'sqft_living15',\n",
        "            'sqft_lot15',\n",
        "        ]\n",
        "        self.discrete = [\n",
        "            'bedrooms',\n",
        "            'bathrooms',\n",
        "            'floors',\n",
        "            'waterfront',\n",
        "            'view',\n",
        "            'condition', # target variable\n",
        "            'grade',\n",
        "        ]\n",
        "        self.integer = [\n",
        "            'price',\n",
        "            'sqft_living',\n",
        "            'sqft_lot',\n",
        "            'sqft_above',\n",
        "            'sqft_basement',\n",
        "            'yr_built',\n",
        "            'yr_renovated',\n",
        "            'sqft_living15',\n",
        "            'sqft_lot15',]\n",
        "        base = base[self.continuous + self.discrete]\n",
        "\n",
        "        self.discrete_dicts = []\n",
        "        self.discrete_dicts_reverse = []\n",
        "        for dis in self.discrete:\n",
        "            discrete_dict = {x:i for i,x in enumerate(sorted(base[dis].unique()))}\n",
        "            self.discrete_dicts_reverse.append({i:x for i,x in enumerate(sorted(base[dis].unique()))})\n",
        "            base[dis] = base[dis].apply(lambda x: discrete_dict.get(x))\n",
        "            self.discrete_dicts.append(discrete_dict)\n",
        "\n",
        "        self.RegTarget = 'long'\n",
        "        self.ClfTarget = 'condition'\n",
        "\n",
        "        # one-hot encoding\n",
        "        df_dummy = []\n",
        "        for d in self.discrete:\n",
        "            df_dummy.append(pd.get_dummies(base[d], prefix=d))\n",
        "        base_dummy = pd.concat([base.drop(columns=self.discrete)] + df_dummy, axis=1)\n",
        "\n",
        "        split_num = 20000\n",
        "\n",
        "        if train:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "\n",
        "            df = base_dummy.iloc[:split_num] # train\n",
        "\n",
        "            self.mean = df[self.continuous].mean(axis=0)\n",
        "            self.std = df[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.train = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)\n",
        "        else:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "            self.test_raw = base.iloc[split_num:]\n",
        "\n",
        "            df_train = base_dummy.iloc[:split_num] # train\n",
        "            df = base_dummy.iloc[split_num:] # test\n",
        "\n",
        "            self.mean = df_train[self.continuous].mean(axis=0)\n",
        "            self.std = df_train[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.test = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)\n",
        "\n",
        "        # Output Information\n",
        "        self.OutputInfo_list = []\n",
        "        for c in self.continuous:\n",
        "            self.OutputInfo_list.append(OutputInfo(1, 'CRPS'))\n",
        "        for d, dummy in zip(self.discrete, df_dummy):\n",
        "            self.OutputInfo_list.append(OutputInfo(dummy.shape[1], 'softmax'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.FloatTensor(self.x_data[idx])\n",
        "        return x\n",
        "''')\n",
        "\n",
        "print(\"Created all dataset modules\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlgrthq8eYuI",
        "outputId": "ce3cdced-2dd5-4eba-fd66-fd24b38e4756"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created all dataset modules\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('modules/cabs_datasets.py', 'w') as f:\n",
        "    f.write('''\n",
        "import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "OutputInfo = namedtuple('OutputInfo', ['dim', 'activation_fn'])\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, train=True):\n",
        "        base = pd.read_csv('./data/sigma_cabs.csv')\n",
        "        base = base.sample(frac=1, random_state=0).reset_index(drop=True)\n",
        "\n",
        "        # Define continuous and discrete columns based on actual data\n",
        "        self.continuous = [\n",
        "            'Trip_Distance', # target variable\n",
        "            'Life_Style_Index',\n",
        "            'Customer_Rating',\n",
        "            'Var1',\n",
        "            'Var2',\n",
        "            'Var3',\n",
        "        ]\n",
        "\n",
        "        self.discrete = [\n",
        "            'Type_of_Cab',\n",
        "            'Customer_Since_Months',\n",
        "            'Confidence_Life_Style_Index',\n",
        "            'Destination_Type',\n",
        "            'Cancellation_Last_1Month',\n",
        "            'Gender',\n",
        "        ]\n",
        "\n",
        "        # Since Surge_Pricing_Type is missing, use Gender as the classification target\n",
        "        self.RegTarget = 'Trip_Distance'\n",
        "        self.ClfTarget = 'Gender'  # Using Gender as alternative classification target\n",
        "\n",
        "        self.integer = [\n",
        "            'Var1',\n",
        "            'Var2',\n",
        "            'Var3']\n",
        "\n",
        "        # Drop any rows with missing values in our selected columns\n",
        "        base = base[self.continuous + self.discrete].dropna()\n",
        "\n",
        "        self.discrete_dicts = []\n",
        "        self.discrete_dicts_reverse = []\n",
        "        for dis in self.discrete:\n",
        "            discrete_dict = {x:i for i,x in enumerate(sorted(base[dis].unique()))}\n",
        "            self.discrete_dicts_reverse.append({i:x for i,x in enumerate(sorted(base[dis].unique()))})\n",
        "            base[dis] = base[dis].apply(lambda x: discrete_dict.get(x))\n",
        "            self.discrete_dicts.append(discrete_dict)\n",
        "\n",
        "        # one-hot encoding\n",
        "        df_dummy = []\n",
        "        for d in self.discrete:\n",
        "            df_dummy.append(pd.get_dummies(base[d], prefix=d))\n",
        "        base_dummy = pd.concat([base.drop(columns=self.discrete)] + df_dummy, axis=1)\n",
        "\n",
        "        # Set split based on data size\n",
        "        split_num = min(int(len(base) * 0.8), 40000)\n",
        "\n",
        "        if train:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "\n",
        "            df = base_dummy.iloc[:split_num] # train\n",
        "\n",
        "            self.mean = df[self.continuous].mean(axis=0)\n",
        "            self.std = df[self.continuous].std(axis=0)\n",
        "\n",
        "            df[self.continuous] = df[self.continuous] - self.mean\n",
        "            df[self.continuous] /= self.std\n",
        "\n",
        "            self.train = df\n",
        "            self.x_data = df.to_numpy().astype(np.float32)\n",
        "        else:\n",
        "            self.train_raw = base.iloc[:split_num]\n",
        "\n",
        "            if len(base) > split_num:\n",
        "                self.test_raw = base.iloc[split_num:]\n",
        "\n",
        "                df_train = base_dummy.iloc[:split_num] # train\n",
        "                df = base_dummy.iloc[split_num:] # test\n",
        "\n",
        "                self.mean = df_train[self.continuous].mean(axis=0)\n",
        "                self.std = df_train[self.continuous].std(axis=0)\n",
        "\n",
        "                df[self.continuous] = df[self.continuous] - self.mean\n",
        "                df[self.continuous] /= self.std\n",
        "\n",
        "                self.test = df\n",
        "                self.x_data = df.to_numpy().astype(np.float32)\n",
        "            else:\n",
        "                # Not enough data for testing, use a small portion of training data\n",
        "                self.test_raw = base.iloc[:min(500, len(base))]\n",
        "\n",
        "                df_train = base_dummy.iloc[:split_num] # train\n",
        "                df = base_dummy.iloc[:min(500, len(base_dummy))] # small test sample\n",
        "\n",
        "                self.mean = df_train[self.continuous].mean(axis=0)\n",
        "                self.std = df_train[self.continuous].std(axis=0)\n",
        "\n",
        "                df[self.continuous] = df[self.continuous] - self.mean\n",
        "                df[self.continuous] /= self.std\n",
        "\n",
        "                self.test = df\n",
        "                self.x_data = df.to_numpy().astype(np.float32)\n",
        "\n",
        "        # Output Information\n",
        "        self.OutputInfo_list = []\n",
        "        for c in self.continuous:\n",
        "            self.OutputInfo_list.append(OutputInfo(1, 'CRPS'))\n",
        "        for d, dummy in zip(self.discrete, df_dummy):\n",
        "            self.OutputInfo_list.append(OutputInfo(dummy.shape[1], 'softmax'))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.FloatTensor(self.x_data[idx])\n",
        "        return x\n",
        "''')\n",
        "\n",
        "print(\"Updated cabs_datasets.py to use the actual columns from your data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0zUTjmaj676",
        "outputId": "32fb0c59-4563-41d0-b512-80fc69df7f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated cabs_datasets.py to use the actual columns from your data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your current directory\n",
        "!pwd\n",
        "\n",
        "# List files to see if simple_train.py exists\n",
        "!ls -la\n",
        "\n",
        "# If the file exists but in a different location, find it\n",
        "!find /content -name \"simple_train.py\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEfHC0x_3ZcD",
        "outputId": "7eef8e15-2df6-4861-88ee-b98c4bff943c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "total 28\n",
            "drwxr-xr-x 1 root root 4096 Apr 24 14:55 .\n",
            "drwxr-xr-x 1 root root 4096 Apr 24 14:49 ..\n",
            "drwxr-xr-x 2 root root 4096 Apr 24 14:55 assets\n",
            "drwxr-xr-x 4 root root 4096 Apr 22 13:37 .config\n",
            "drwxr-xr-x 2 root root 4096 Apr 24 14:57 data\n",
            "drwxr-xr-x 2 root root 4096 Apr 24 14:58 modules\n",
            "drwxr-xr-x 1 root root 4096 Apr 22 13:37 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create simple_train.py file\n",
        "with open('simple_train.py', 'w') as f:\n",
        "    f.write('''\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from modules.simulation import set_random_seed\n",
        "from modules.model import VAE\n",
        "from modules.train import train_VAE\n",
        "\n",
        "# Configuration\n",
        "import argparse\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser('parameters')\n",
        "\n",
        "    parser.add_argument('--seed', type=int, default=1,\n",
        "                        help='seed for repeatable results')\n",
        "    parser.add_argument('--dataset', type=str, default='adult',\n",
        "                        help='Dataset options')\n",
        "\n",
        "    parser.add_argument(\"--latent_dim\", default=2, type=int,\n",
        "                        help=\"the latent dimension size\")\n",
        "    parser.add_argument(\"--step\", default=0.1, type=float,\n",
        "                        help=\"interval size of quantile levels\")\n",
        "\n",
        "    parser.add_argument('--epochs', default=10, type=int,\n",
        "                        help='the number of epochs')\n",
        "    parser.add_argument('--batch_size', default=256, type=int,\n",
        "                        help='batch size')\n",
        "    parser.add_argument('--lr', default=1e-3, type=float,\n",
        "                        help='learning rate')\n",
        "    parser.add_argument('--threshold', default=1e-5, type=float,\n",
        "                        help='threshold for clipping alpha_tilde')\n",
        "\n",
        "    parser.add_argument('--beta', default=0.5, type=float,\n",
        "                        help='scale parameter')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    # Get configuration\n",
        "    config = vars(get_args())\n",
        "    config[\"cuda\"] = torch.cuda.is_available()\n",
        "    device = torch.device('cuda:0') if config[\"cuda\"] else torch.device('cpu')\n",
        "\n",
        "    print(\"Configuration:\")\n",
        "    for k, v in config.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "    # Set random seed\n",
        "    set_random_seed(config[\"seed\"])\n",
        "    torch.manual_seed(config[\"seed\"])\n",
        "    if config[\"cuda\"]:\n",
        "        torch.cuda.manual_seed(config[\"seed\"])\n",
        "\n",
        "    # Import dataset module\n",
        "    import importlib\n",
        "    dataset_module = importlib.import_module(f'modules.{config[\"dataset\"]}_datasets')\n",
        "    TabularDataset = dataset_module.TabularDataset\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = TabularDataset()\n",
        "    dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "\n",
        "    # Update config with dataset dimensions\n",
        "    OutputInfo_list = dataset.OutputInfo_list\n",
        "    CRPS_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'CRPS'])\n",
        "    softmax_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'softmax'])\n",
        "    config[\"CRPS_dim\"] = CRPS_dim\n",
        "    config[\"softmax_dim\"] = softmax_dim\n",
        "\n",
        "    print(f\"\\\\nDataset: {config['dataset']}\")\n",
        "    print(f\"CRPS dimensions: {CRPS_dim}\")\n",
        "    print(f\"Softmax dimensions: {softmax_dim}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = VAE(config, device).to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config[\"lr\"]\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\\\nStarting training...\")\n",
        "    for epoch in range(config[\"epochs\"]):\n",
        "        logs = train_VAE(OutputInfo_list, dataloader, model, config, optimizer, device)\n",
        "\n",
        "        print_input = f\"[epoch {epoch + 1:03d}]\"\n",
        "        print_input += ''.join([f\", {x}: {np.mean(y):.4f}\" for x, y in logs.items()])\n",
        "        print(print_input)\n",
        "\n",
        "    # Save model\n",
        "    print(\"\\\\nSaving model...\")\n",
        "    os.makedirs('./assets', exist_ok=True)\n",
        "    torch.save(model.state_dict(), f'./assets/DistVAE_{config[\"dataset\"]}.pth')\n",
        "\n",
        "    # Generate synthetic data\n",
        "    print(\"\\\\nGenerating synthetic data...\")\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        n = len(dataset.train)\n",
        "        synthetic_data = model.generate_data(n, OutputInfo_list, dataset, reverse_col=True)\n",
        "\n",
        "    # Save synthetic data\n",
        "    synthetic_data.to_csv(f'./assets/synthetic_{config[\"dataset\"]}.csv', index=False)\n",
        "    print(f\"Synthetic data saved to ./assets/synthetic_{config['dataset']}.csv\")\n",
        "\n",
        "    # Print sample of synthetic data\n",
        "    print(\"\\\\nSample of synthetic data:\")\n",
        "    print(synthetic_data.head())\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"Created simple_train.py file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2ANkYJa38RV",
        "outputId": "d5f5acac-1fc0-4219-8eec-7c4bd25a7272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created simple_train.py file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GENERATING SYNTHEIC DATA FOR ALL DATASET**"
      ],
      "metadata": {
        "id": "BeuQPif-bMhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ADULTS_DATASET**"
      ],
      "metadata": {
        "id": "euBaygn7e6gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_train.py --dataset adult --epochs 10 --batch_size 256 --latent_dim 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCpN0w9ObCko",
        "outputId": "6d70acdb-2881-41ea-f7c8-96770ed786d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: adult\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 10\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "/content/modules/adult_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "\n",
            "Dataset: adult\n",
            "CRPS dimensions: 5\n",
            "Softmax dimensions: 100\n",
            "\n",
            "Starting training...\n",
            "inner loop: 100% 157/157 [00:04<00:00, 38.73it/s]\n",
            "[epoch 001], loss: 13.6668, quantile: 13.5881, KL: 0.1573, activated: 0.0000\n",
            "inner loop: 100% 157/157 [00:02<00:00, 54.56it/s]\n",
            "[epoch 002], loss: 10.4615, quantile: 9.9944, KL: 0.9342, activated: 0.0000\n",
            "inner loop: 100% 157/157 [00:02<00:00, 56.89it/s]\n",
            "[epoch 003], loss: 9.7399, quantile: 8.8930, KL: 1.6938, activated: 0.1465\n",
            "inner loop: 100% 157/157 [00:02<00:00, 54.01it/s]\n",
            "[epoch 004], loss: 9.3815, quantile: 8.3178, KL: 2.1274, activated: 0.9873\n",
            "inner loop: 100% 157/157 [00:02<00:00, 56.50it/s]\n",
            "[epoch 005], loss: 9.2666, quantile: 8.1494, KL: 2.2344, activated: 1.0000\n",
            "inner loop: 100% 157/157 [00:02<00:00, 55.14it/s]\n",
            "[epoch 006], loss: 9.2120, quantile: 8.0437, KL: 2.3365, activated: 1.0000\n",
            "inner loop: 100% 157/157 [00:02<00:00, 56.78it/s]\n",
            "[epoch 007], loss: 9.1379, quantile: 7.9308, KL: 2.4142, activated: 1.0000\n",
            "inner loop: 100% 157/157 [00:02<00:00, 55.99it/s]\n",
            "[epoch 008], loss: 9.0878, quantile: 7.8453, KL: 2.4848, activated: 1.0000\n",
            "inner loop: 100% 157/157 [00:02<00:00, 53.81it/s]\n",
            "[epoch 009], loss: 9.0275, quantile: 7.7432, KL: 2.5685, activated: 1.0000\n",
            "inner loop: 100% 157/157 [00:02<00:00, 56.42it/s]\n",
            "[epoch 010], loss: 8.9412, quantile: 7.6199, KL: 2.6426, activated: 1.0000\n",
            "\n",
            "Saving model...\n",
            "\n",
            "Generating synthetic data...\n",
            "Synthetic data saved to ./assets/synthetic_adult.csv\n",
            "\n",
            "Sample of synthetic data:\n",
            "   age  educational-num  capital-gain  ...  gender  native-country income\n",
            "0   20                9          -181  ...  Female   United-States  <=50K\n",
            "1   31               10           -66  ...    Male   United-States  <=50K\n",
            "2   48               10           -12  ...    Male   United-States   >50K\n",
            "3   23               14           -20  ...  Female   United-States  <=50K\n",
            "4   56               12          1871  ...    Male   United-States   >50K\n",
            "\n",
            "[5 rows x 14 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0U_rNG66fK8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **COVOTYPE DATASET**"
      ],
      "metadata": {
        "id": "eaWP5qKlfOOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_train.py --dataset covtype --epochs 10 --batch_size 256 --latent_dim 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lGY9cQ2e4zI",
        "outputId": "75c0f84d-f52b-4dd2-af2d-3870dd96e30f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: covtype\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 10\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "/content/modules/covtype_datasets.py:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "\n",
            "Dataset: covtype\n",
            "CRPS dimensions: 10\n",
            "Softmax dimensions: 7\n",
            "\n",
            "Starting training...\n",
            "inner loop: 100% 176/176 [00:04<00:00, 36.22it/s]\n",
            "[epoch 001], loss: 4.4516, quantile: 4.4400, KL: 0.0232, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 38.82it/s]\n",
            "[epoch 002], loss: 3.9769, quantile: 3.9742, KL: 0.0055, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 36.79it/s]\n",
            "[epoch 003], loss: 3.9354, quantile: 3.8933, KL: 0.0842, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 40.28it/s]\n",
            "[epoch 004], loss: 3.6778, quantile: 3.3367, KL: 0.6823, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 38.39it/s]\n",
            "[epoch 005], loss: 3.5099, quantile: 2.9918, KL: 1.0361, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 40.40it/s]\n",
            "[epoch 006], loss: 3.4494, quantile: 2.8250, KL: 1.2488, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 40.38it/s]\n",
            "[epoch 007], loss: 3.4055, quantile: 2.7219, KL: 1.3672, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 37.51it/s]\n",
            "[epoch 008], loss: 3.3826, quantile: 2.6585, KL: 1.4482, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 39.64it/s]\n",
            "[epoch 009], loss: 3.3560, quantile: 2.5967, KL: 1.5186, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 40.83it/s]\n",
            "[epoch 010], loss: 3.3365, quantile: 2.5442, KL: 1.5846, activated: 0.0000\n",
            "\n",
            "Saving model...\n",
            "\n",
            "Generating synthetic data...\n",
            "Synthetic data saved to ./assets/synthetic_covtype.csv\n",
            "\n",
            "Sample of synthetic data:\n",
            "   Elevation  Aspect  ...  Horizontal_Distance_To_Fire_Points  Cover_Type\n",
            "0       2745      53  ...                                 629           2\n",
            "1       3019     192  ...                                1501           1\n",
            "2       3226     296  ...                                2394           1\n",
            "3       2739     151  ...                                1511           2\n",
            "4       3288      27  ...                                2342           1\n",
            "\n",
            "[5 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREDIT DATASET**"
      ],
      "metadata": {
        "id": "lDwRBQAtgD_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_train.py --dataset credit --epochs 10 --batch_size 256 --latent_dim 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y63WjVuNgDYS",
        "outputId": "efe0687c-a471-4de4-989e-2c258cc6e18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: credit\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 10\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "\n",
            "Dataset: credit\n",
            "CRPS dimensions: 10\n",
            "Softmax dimensions: 37\n",
            "\n",
            "Starting training...\n",
            "inner loop: 100% 67/67 [00:02<00:00, 30.54it/s]\n",
            "[epoch 001], loss: 12.0883, quantile: 12.0594, KL: 0.0577, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 38.14it/s]\n",
            "[epoch 002], loss: 8.7645, quantile: 8.7473, KL: 0.0344, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 39.31it/s]\n",
            "[epoch 003], loss: 8.4560, quantile: 8.4461, KL: 0.0198, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 36.28it/s]\n",
            "[epoch 004], loss: 8.4034, quantile: 8.3841, KL: 0.0386, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 36.01it/s]\n",
            "[epoch 005], loss: 8.3685, quantile: 8.3342, KL: 0.0687, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 37.61it/s]\n",
            "[epoch 006], loss: 8.3358, quantile: 8.2751, KL: 0.1214, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:02<00:00, 32.83it/s]\n",
            "[epoch 007], loss: 8.3198, quantile: 8.2220, KL: 0.1955, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 35.29it/s]\n",
            "[epoch 008], loss: 8.2365, quantile: 8.0448, KL: 0.3834, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 34.58it/s]\n",
            "[epoch 009], loss: 8.1644, quantile: 7.8845, KL: 0.5599, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:02<00:00, 31.74it/s]\n",
            "[epoch 010], loss: 8.1124, quantile: 7.7831, KL: 0.6587, activated: 0.0000\n",
            "\n",
            "Saving model...\n",
            "\n",
            "Generating synthetic data...\n",
            "Synthetic data saved to ./assets/synthetic_credit.csv\n",
            "\n",
            "Sample of synthetic data:\n",
            "   AMT_INCOME_TOTAL    AMT_CREDIT  ...  NAME_HOUSING_TYPE  TARGET\n",
            "0     215553.791303  3.880631e+05  ...  House / apartment       0\n",
            "1     118154.335667  5.433507e+05  ...  House / apartment       0\n",
            "2     234451.336726  6.778064e+05  ...  House / apartment       0\n",
            "3     121215.371791  1.627788e+06  ...  House / apartment       0\n",
            "4     267687.981393  5.110704e+05  ...  House / apartment       0\n",
            "\n",
            "[5 rows x 19 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOAN DATASET**"
      ],
      "metadata": {
        "id": "hzjVa_M_gfXZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_train.py --dataset loan --epochs 10 --batch_size 256 --latent_dim 2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWTGZ3OmgeO2",
        "outputId": "e91705d1-b11c-4c72-9771-a1ec554f607c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: loan\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 10\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "/content/modules/loan_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "\n",
            "Dataset: loan\n",
            "CRPS dimensions: 5\n",
            "Softmax dimensions: 14\n",
            "\n",
            "Starting training...\n",
            "inner loop: 100% 16/16 [00:00<00:00, 21.76it/s]\n",
            "[epoch 001], loss: 6.9936, quantile: 6.9689, KL: 0.0495, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 58.97it/s]\n",
            "[epoch 002], loss: 6.0405, quantile: 6.0256, KL: 0.0297, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 65.09it/s]\n",
            "[epoch 003], loss: 5.3765, quantile: 5.3602, KL: 0.0327, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 63.21it/s]\n",
            "[epoch 004], loss: 5.1235, quantile: 5.1087, KL: 0.0295, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 60.35it/s]\n",
            "[epoch 005], loss: 5.0338, quantile: 5.0248, KL: 0.0179, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 63.72it/s]\n",
            "[epoch 006], loss: 4.9681, quantile: 4.9615, KL: 0.0130, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 62.68it/s]\n",
            "[epoch 007], loss: 4.9562, quantile: 4.9506, KL: 0.0113, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 65.01it/s]\n",
            "[epoch 008], loss: 4.9370, quantile: 4.9318, KL: 0.0105, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 63.19it/s]\n",
            "[epoch 009], loss: 4.9211, quantile: 4.9158, KL: 0.0106, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 67.10it/s]\n",
            "[epoch 010], loss: 4.9087, quantile: 4.9030, KL: 0.0115, activated: 0.0000\n",
            "\n",
            "Saving model...\n",
            "\n",
            "Generating synthetic data...\n",
            "Synthetic data saved to ./assets/synthetic_loan.csv\n",
            "\n",
            "Sample of synthetic data:\n",
            "   Age  Experience  Income  ...  CD Account  Online  CreditCard\n",
            "0   48           9     100  ...           0       0           1\n",
            "1   35          17      52  ...           0       0           0\n",
            "2   52          21     136  ...           0       0           0\n",
            "3   36          36      14  ...           0       1           0\n",
            "4   64          26     107  ...           0       1           0\n",
            "\n",
            "[5 rows x 11 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Check the actual columns in the cabs dataset\n",
        "df = pd.read_csv('./data/sigma_cabs.csv')\n",
        "print(\"Columns in sigma_cabs.csv:\")\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QpFyrQ3hyPL",
        "outputId": "71678da5-22c0-4dc9-9391-c61d6d91cc12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in sigma_cabs.csv:\n",
            "['Trip_ID', 'Trip_Distance', 'Type_of_Cab', 'Customer_Since_Months', 'Life_Style_Index', 'Confidence_Life_Style_Index', 'Destination_Type', 'Customer_Rating', 'Cancellation_Last_1Month', 'Var1', 'Var2', 'Var3', 'Gender']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **cabs dataset**"
      ],
      "metadata": {
        "id": "sXe6qnRpgpve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_train.py --dataset cabs --epochs 10 --batch_size 256 --latent_dim 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MU1Dn4EgpHe",
        "outputId": "4781fb9c-fc49-4f7a-b6a4-c64c74996991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: cabs\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 10\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "/content/modules/cabs_datasets.py:78: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:79: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "\n",
            "Dataset: cabs\n",
            "CRPS dimensions: 6\n",
            "Softmax dimensions: 44\n",
            "\n",
            "Starting training...\n",
            "inner loop: 100% 88/88 [00:02<00:00, 43.00it/s]\n",
            "[epoch 001], loss: 11.3605, quantile: 11.3311, KL: 0.0588, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 53.56it/s]\n",
            "[epoch 002], loss: 9.7377, quantile: 9.7221, KL: 0.0313, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 53.73it/s]\n",
            "[epoch 003], loss: 9.6259, quantile: 9.6117, KL: 0.0285, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 53.57it/s]\n",
            "[epoch 004], loss: 9.5659, quantile: 9.5017, KL: 0.1285, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 51.69it/s]\n",
            "[epoch 005], loss: 9.4379, quantile: 9.2117, KL: 0.4523, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 52.56it/s]\n",
            "[epoch 006], loss: 9.3663, quantile: 9.0474, KL: 0.6378, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 52.61it/s]\n",
            "[epoch 007], loss: 9.2455, quantile: 8.7633, KL: 0.9645, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 54.07it/s]\n",
            "[epoch 008], loss: 9.1246, quantile: 8.4836, KL: 1.2819, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 54.15it/s]\n",
            "[epoch 009], loss: 9.0643, quantile: 8.3479, KL: 1.4328, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 51.43it/s]\n",
            "[epoch 010], loss: 9.0269, quantile: 8.2684, KL: 1.5171, activated: 0.0000\n",
            "\n",
            "Saving model...\n",
            "\n",
            "Generating synthetic data...\n",
            "Synthetic data saved to ./assets/synthetic_cabs.csv\n",
            "\n",
            "Sample of synthetic data:\n",
            "   Trip_Distance  Life_Style_Index  ...  Cancellation_Last_1Month  Gender\n",
            "0      33.177236          2.616023  ...                         1    Male\n",
            "1      25.064161          2.822658  ...                         0    Male\n",
            "2      62.015939          2.886378  ...                         0    Male\n",
            "3      10.113106          3.099506  ...                         2    Male\n",
            "4      81.365064          2.989249  ...                         1    Male\n",
            "\n",
            "[5 rows x 12 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_train.py --dataset kings --epochs 10 --batch_size 256 --latent_dim 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe2YlE01kKB-",
        "outputId": "577c03d6-7b3b-4cc5-8afa-4da6dd30c24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: kings\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 10\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "/content/modules/kings_datasets.py:83: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "\n",
            "Dataset: kings\n",
            "CRPS dimensions: 11\n",
            "Softmax dimensions: 73\n",
            "\n",
            "Starting training...\n",
            "inner loop: 100% 79/79 [00:02<00:00, 30.23it/s]\n",
            "[epoch 001], loss: 14.0153, quantile: 13.9565, KL: 0.1176, activated: 0.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 34.47it/s]\n",
            "[epoch 002], loss: 10.0705, quantile: 9.9889, KL: 0.1631, activated: 0.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 34.84it/s]\n",
            "[epoch 003], loss: 9.1887, quantile: 8.7534, KL: 0.8707, activated: 0.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 35.59it/s]\n",
            "[epoch 004], loss: 8.6069, quantile: 7.9870, KL: 1.2398, activated: 0.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 36.70it/s]\n",
            "[epoch 005], loss: 8.2881, quantile: 7.4843, KL: 1.6077, activated: 0.8987\n",
            "inner loop: 100% 79/79 [00:02<00:00, 36.38it/s]\n",
            "[epoch 006], loss: 8.1099, quantile: 7.1797, KL: 1.8605, activated: 1.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 33.16it/s]\n",
            "[epoch 007], loss: 7.9306, quantile: 6.9174, KL: 2.0263, activated: 1.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 34.75it/s]\n",
            "[epoch 008], loss: 7.8588, quantile: 6.7984, KL: 2.1207, activated: 1.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 34.24it/s]\n",
            "[epoch 009], loss: 7.7879, quantile: 6.6901, KL: 2.1956, activated: 1.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 33.63it/s]\n",
            "[epoch 010], loss: 7.7397, quantile: 6.6169, KL: 2.2457, activated: 1.0000\n",
            "\n",
            "Saving model...\n",
            "\n",
            "Generating synthetic data...\n",
            "Synthetic data saved to ./assets/synthetic_kings.csv\n",
            "\n",
            "Sample of synthetic data:\n",
            "     price  sqft_living  sqft_lot  ...  view  condition  grade\n",
            "0   374700         1052      4887  ...     0          3      7\n",
            "1   358331         2093      7960  ...     0          3      8\n",
            "2   672956         2443     20901  ...     0          4      7\n",
            "3   180592         1410      1599  ...     0          3      7\n",
            "4  1385553         2940     12186  ...     0          4      7\n",
            "\n",
            "[5 rows x 18 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUNNING INFERENCE SCRIPT**"
      ],
      "metadata": {
        "id": "0ArfG_JqyNBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix the simple_inference.py script\n",
        "with open('simple_inference.py', 'w') as f:\n",
        "    f.write('''\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from modules.simulation import set_random_seed\n",
        "from modules.model import VAE\n",
        "\n",
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "from scipy import interpolate\n",
        "\n",
        "# Configuration\n",
        "import argparse\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser('parameters')\n",
        "\n",
        "    parser.add_argument('--dataset', type=str, default='adult',\n",
        "                        help='Dataset options: covtype, credit, loan, adult, cabs, kings')\n",
        "    parser.add_argument('--beta', default=0.5, type=float,\n",
        "                        help='observation noise')\n",
        "    parser.add_argument(\"--latent_dim\", default=2, type=int,\n",
        "                        help=\"the latent dimension size\")\n",
        "    parser.add_argument(\"--step\", default=0.1, type=float,\n",
        "                        help=\"interval size of quantile levels\")\n",
        "    parser.add_argument('--threshold', default=1e-5, type=float,\n",
        "                        help='threshold for clipping alpha_tilde')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    # Get configuration\n",
        "    args = get_args()\n",
        "    config = vars(args)\n",
        "\n",
        "    # Setup directories\n",
        "    if not os.path.exists(f'./assets/{config[\"dataset\"]}'):\n",
        "        os.makedirs(f'./assets/{config[\"dataset\"]}')\n",
        "\n",
        "    config[\"cuda\"] = torch.cuda.is_available()\n",
        "    device = torch.device('cuda:0') if config[\"cuda\"] else torch.device('cpu')\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Evaluating dataset: {config['dataset']}\")\n",
        "\n",
        "    # Set random seed\n",
        "    set_random_seed(1)\n",
        "    torch.manual_seed(1)\n",
        "    if config[\"cuda\"]:\n",
        "        torch.cuda.manual_seed(1)\n",
        "\n",
        "    # Import dataset module\n",
        "    import importlib\n",
        "    dataset_module = importlib.import_module(f'modules.{config[\"dataset\"]}_datasets')\n",
        "    TabularDataset = dataset_module.TabularDataset\n",
        "\n",
        "    dataset = TabularDataset()\n",
        "    test_dataset = TabularDataset(train=False)\n",
        "\n",
        "    # Update config with dataset dimensions\n",
        "    OutputInfo_list = dataset.OutputInfo_list\n",
        "    CRPS_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'CRPS'])\n",
        "    softmax_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'softmax'])\n",
        "    config[\"CRPS_dim\"] = CRPS_dim\n",
        "    config[\"softmax_dim\"] = softmax_dim\n",
        "\n",
        "    # Initialize model\n",
        "    model = VAE(config, device).to(device)\n",
        "\n",
        "    # Load model\n",
        "    model_path = f'./assets/DistVAE_{config[\"dataset\"]}.pth'\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Quantile Estimation with sampling mechanism\n",
        "    print(\"Evaluating quantile estimation...\")\n",
        "\n",
        "    n = 100\n",
        "    MC = 1000  # Monte Carlo samples\n",
        "    x_linspace = np.linspace(\n",
        "        [np.min(dataset.x_data[:, k]) for k in range(len(dataset.continuous))],\n",
        "        [np.max(dataset.x_data[:, k]) for k in range(len(dataset.continuous))],\n",
        "        n)\n",
        "    x_linspace = torch.from_numpy(x_linspace).to(device).float()\n",
        "\n",
        "    alpha_hat = torch.zeros((n, len(dataset.continuous)), device=device)\n",
        "    for _ in tqdm.tqdm(range(MC), desc=\"Estimate CDF...\"):\n",
        "        randn = torch.randn(n, config[\"latent_dim\"], device=device)\n",
        "        with torch.no_grad():\n",
        "            gamma, beta, _ = model.quantile_parameter(randn)\n",
        "            alpha_tilde_list = model.quantile_inverse(x_linspace, gamma, beta)\n",
        "            alpha_hat += torch.cat(alpha_tilde_list, dim=1)\n",
        "    alpha_hat /= MC\n",
        "\n",
        "    # Alpha-rate\n",
        "    alpha_levels = np.array([0.1, 0.3, 0.5, 0.7, 0.9])\n",
        "    alpha_rate = []\n",
        "\n",
        "    # Move to CPU for numpy operations\n",
        "    alpha_hat_cpu = alpha_hat.cpu().numpy()\n",
        "    x_linspace_cpu = x_linspace.cpu().numpy()\n",
        "\n",
        "    for j in range(len(dataset.continuous)):\n",
        "        tmp = []\n",
        "        for alpha in alpha_levels:\n",
        "            if len(np.where(alpha_hat_cpu[:, j] < alpha)[0]):\n",
        "                cut1 = np.where(alpha_hat_cpu[:, j] < alpha)[0][-1]\n",
        "            else:\n",
        "                cut1 = 0\n",
        "            if len(np.where(alpha < alpha_hat_cpu[:, j])[0]):\n",
        "                cut2 = np.where(alpha < alpha_hat_cpu[:, j])[0][0]\n",
        "            else:\n",
        "                cut2 = -1\n",
        "\n",
        "            f_inter = interpolate.interp1d(\n",
        "                [alpha_hat_cpu[cut1, j], alpha_hat_cpu[cut2, j]],\n",
        "                [x_linspace_cpu[:, j][cut1], x_linspace_cpu[:, j][cut2]])\n",
        "            try:\n",
        "                tmp.append((test_dataset.x_data[:, j] <= f_inter(alpha)).mean())\n",
        "            except:\n",
        "                tmp.append((test_dataset.x_data[:, j] <= x_linspace_cpu[:, j][cut2]).mean())\n",
        "        alpha_rate.append(tmp)\n",
        "\n",
        "    alpha_rate = np.array(alpha_rate).mean(axis=0)\n",
        "\n",
        "    # Save alpha_rate results\n",
        "    pd.DataFrame(\n",
        "        np.concatenate([\n",
        "            alpha_rate[None, :],\n",
        "            np.abs(alpha_rate - alpha_levels)[None, :]\n",
        "        ], axis=0).round(3),\n",
        "        columns=[str(x) for x in alpha_levels]\n",
        "    ).to_csv(f'./assets/{config[\"dataset\"]}/{config[\"dataset\"]}_alpha_rate.csv')\n",
        "\n",
        "    # Visualize CDFs\n",
        "    # Adjust figure layout based on dataset\n",
        "    if config[\"dataset\"] in [\"covtype\", \"credit\"]:\n",
        "        fig, ax = plt.subplots(2, CRPS_dim // 2,\n",
        "                               figsize=(3 * CRPS_dim // 2, 3 * 2))\n",
        "    elif config[\"dataset\"] in [\"loan\", \"adult\", \"cabs\"]:\n",
        "        fig, ax = plt.subplots(1, CRPS_dim,\n",
        "                               figsize=(3 * CRPS_dim, 3 * 1))\n",
        "    elif config[\"dataset\"] == \"kings\":\n",
        "        fig, ax = plt.subplots(2, CRPS_dim // 2 + 1,\n",
        "                               figsize=(3 * CRPS_dim // 2 + 1, 3 * 2))\n",
        "    else:\n",
        "        fig, ax = plt.subplots(1, CRPS_dim, figsize=(3 * CRPS_dim, 3))\n",
        "\n",
        "    # Get original data in original scale\n",
        "    orig = dataset.x_data[:, :len(dataset.continuous)] * np.array(dataset.std)\n",
        "    orig += np.array(dataset.mean)\n",
        "    orig = pd.DataFrame(orig, columns=dataset.continuous)\n",
        "    if hasattr(dataset, 'integer'):\n",
        "        orig[dataset.integer] = orig[dataset.integer].astype(int)\n",
        "\n",
        "    for k, v in enumerate(dataset.continuous):\n",
        "        x_linspace_orig = [np.arange(x, y, 1) for x, y in zip(\n",
        "            [np.min(orig.to_numpy()[:, k])],\n",
        "            [np.max(orig.to_numpy()[:, k])])][0]\n",
        "\n",
        "        if hasattr(dataset, 'integer') and v in dataset.integer:\n",
        "            ecdf = ECDF(orig[dataset.continuous].to_numpy()[:, k])\n",
        "            emp = [ecdf(x) for x in x_linspace_orig]\n",
        "            ax.flatten()[k].step(\n",
        "                (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
        "                emp, where='post',\n",
        "                label=\"empirical\", linewidth=3.5, color=u'#ff7f0e')\n",
        "        else:\n",
        "            q = np.linspace(0, 1, 100)\n",
        "            ax.flatten()[k].step(\n",
        "                np.quantile(dataset.x_data[:, k], q=q),\n",
        "                q, where='post',\n",
        "                label=\"empirical\", linewidth=3.5, color=u'#ff7f0e')\n",
        "\n",
        "        ax.flatten()[k].plot(\n",
        "            x_linspace_cpu[:, k], alpha_hat_cpu[:, k],\n",
        "            label=\"estimate\", linewidth=3.5, linestyle='dashed', color=u'#1f77b4')\n",
        "\n",
        "        ax.flatten()[k].set_xlabel(v, fontsize=12)\n",
        "        ax.flatten()[k].tick_params(axis=\"x\", labelsize=14)\n",
        "        ax.flatten()[k].tick_params(axis=\"y\", labelsize=14)\n",
        "\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'./assets/{config[\"dataset\"]}/{config[\"dataset\"]}_estimated_quantile.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"Results saved to ./assets/{config['dataset']}/\")\n",
        "    print(\"Inference evaluation complete!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"Fixed simple_inference.py by adding missing config parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATpzeNydu3Ob",
        "outputId": "b4c7e526-dc2b-4fe6-cfca-22627fc8d92c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed simple_inference.py by adding missing config parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in ['adult', 'covtype', 'credit', 'loan', 'cabs', 'kings']:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Running inference for dataset: {dataset}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    !python simple_inference.py --dataset {dataset}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9STvD3HcvId3",
        "outputId": "2c43c728-9605-4961-a1db-3bb8415f7787",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Running inference for dataset: adult\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Evaluating dataset: adult\n",
            "/content/modules/adult_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/adult_datasets.py:87: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:88: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_adult.pth\n",
            "Evaluating quantile estimation...\n",
            "Estimate CDF...: 100% 1000/1000 [00:03<00:00, 300.04it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/interpolate/_interpolate.py:479: RuntimeWarning: invalid value encountered in divide\n",
            "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "Results saved to ./assets/adult/\n",
            "Inference evaluation complete!\n",
            "\n",
            "==================================================\n",
            "Running inference for dataset: covtype\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Evaluating dataset: covtype\n",
            "/content/modules/covtype_datasets.py:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/covtype_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:85: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_covtype.pth\n",
            "Evaluating quantile estimation...\n",
            "Estimate CDF...: 100% 1000/1000 [00:06<00:00, 163.45it/s]\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "Results saved to ./assets/covtype/\n",
            "Inference evaluation complete!\n",
            "\n",
            "==================================================\n",
            "Running inference for dataset: credit\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Evaluating dataset: credit\n",
            "Loading model from ./assets/DistVAE_credit.pth\n",
            "Evaluating quantile estimation...\n",
            "Estimate CDF...: 100% 1000/1000 [00:06<00:00, 153.85it/s]\n",
            "/content/simple_inference.py:126: RuntimeWarning: Mean of empty slice.\n",
            "  tmp.append((test_dataset.x_data[:, j] <= f_inter(alpha)).mean())\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "Results saved to ./assets/credit/\n",
            "Inference evaluation complete!\n",
            "\n",
            "==================================================\n",
            "Running inference for dataset: loan\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Evaluating dataset: loan\n",
            "/content/modules/loan_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/loan_datasets.py:87: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:88: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_loan.pth\n",
            "Evaluating quantile estimation...\n",
            "Estimate CDF...: 100% 1000/1000 [00:03<00:00, 280.92it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/interpolate/_interpolate.py:479: RuntimeWarning: invalid value encountered in divide\n",
            "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "Results saved to ./assets/loan/\n",
            "Inference evaluation complete!\n",
            "\n",
            "==================================================\n",
            "Running inference for dataset: cabs\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Evaluating dataset: cabs\n",
            "/content/modules/cabs_datasets.py:78: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:79: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/cabs_datasets.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:96: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_cabs.pth\n",
            "Evaluating quantile estimation...\n",
            "Estimate CDF...: 100% 1000/1000 [00:04<00:00, 245.24it/s]\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "Results saved to ./assets/cabs/\n",
            "Inference evaluation complete!\n",
            "\n",
            "==================================================\n",
            "Running inference for dataset: kings\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Evaluating dataset: kings\n",
            "/content/modules/kings_datasets.py:83: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/kings_datasets.py:98: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:99: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_kings.pth\n",
            "Evaluating quantile estimation...\n",
            "Estimate CDF...: 100% 1000/1000 [00:07<00:00, 136.00it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/scipy/interpolate/_interpolate.py:479: RuntimeWarning: invalid value encountered in divide\n",
            "  slope = (y_hi - y_lo) / (x_hi - x_lo)[:, None]\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:172: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  (x_linspace_orig - dataset.mean[k]) / dataset.std[k],\n",
            "/content/simple_inference.py:190: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
            "  plt.legend()\n",
            "Results saved to ./assets/kings/\n",
            "Inference evaluation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CALIBRATING THE DATA**"
      ],
      "metadata": {
        "id": "7ItBKe0pyZci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create simple_calibration.py\n",
        "with open('simple_calibration.py', 'w') as f:\n",
        "    f.write('''\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from modules.simulation import set_random_seed\n",
        "from modules.model import VAE\n",
        "\n",
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "\n",
        "# Configuration\n",
        "import argparse\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser('parameters')\n",
        "\n",
        "    parser.add_argument('--dataset', type=str, default='adult',\n",
        "                        help='Dataset options: covtype, credit, loan, adult, cabs, kings')\n",
        "    parser.add_argument(\"--latent_dim\", default=2, type=int,\n",
        "                        help=\"the latent dimension size\")\n",
        "    parser.add_argument(\"--step\", default=0.1, type=float,\n",
        "                        help=\"interval size of quantile levels\")\n",
        "    parser.add_argument('--threshold', default=1e-5, type=float,\n",
        "                        help='threshold for clipping alpha_tilde')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    # Get configuration\n",
        "    args = get_args()\n",
        "    config = vars(args)\n",
        "\n",
        "    # Setup\n",
        "    if not os.path.exists(f'./assets/{config[\"dataset\"]}'):\n",
        "        os.makedirs(f'./assets/{config[\"dataset\"]}')\n",
        "\n",
        "    config[\"cuda\"] = torch.cuda.is_available()\n",
        "    device = torch.device('cuda:0') if config[\"cuda\"] else torch.device('cpu')\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Calibrating dataset: {config['dataset']}\")\n",
        "\n",
        "    # Set random seed\n",
        "    set_random_seed(1)\n",
        "    torch.manual_seed(1)\n",
        "    if config[\"cuda\"]:\n",
        "        torch.cuda.manual_seed(1)\n",
        "\n",
        "    # Import dataset module\n",
        "    import importlib\n",
        "    dataset_module = importlib.import_module(f'modules.{config[\"dataset\"]}_datasets')\n",
        "    TabularDataset = dataset_module.TabularDataset\n",
        "\n",
        "    dataset = TabularDataset()\n",
        "\n",
        "    # Update config with dataset dimensions\n",
        "    OutputInfo_list = dataset.OutputInfo_list\n",
        "    CRPS_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'CRPS'])\n",
        "    softmax_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'softmax'])\n",
        "    config[\"CRPS_dim\"] = CRPS_dim\n",
        "    config[\"softmax_dim\"] = softmax_dim\n",
        "\n",
        "    # Initialize model\n",
        "    model = VAE(config, device).to(device)\n",
        "\n",
        "    # Load model\n",
        "    model_path = f'./assets/DistVAE_{config[\"dataset\"]}.pth'\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Get raw data for calibration\n",
        "    df = dataset.train_raw[dataset.continuous]\n",
        "\n",
        "    # Select a continuous variable for calibration demonstration\n",
        "    j = 1 if len(dataset.continuous) > 1 else 0  # Use second continuous var if available\n",
        "    print(f\"Calibrating {dataset.continuous[j]}\")\n",
        "\n",
        "    # Monte Carlo estimation\n",
        "    MC = 1000  # Monte Carlo samples\n",
        "\n",
        "    # Step 0: Estimate CDF for various x values (for demonstration)\n",
        "    n = 100\n",
        "    x_linspace_est = np.linspace(\n",
        "        np.min(dataset.x_data[:, j]),\n",
        "        np.max(dataset.x_data[:, j]),\n",
        "        n)\n",
        "\n",
        "    print(\"Estimating CDF...\")\n",
        "    alpha_est = torch.zeros((len(x_linspace_est), 1), device=device)\n",
        "    for _ in tqdm.tqdm(range(MC), desc=\"Estimate CDF...\"):\n",
        "        randn = torch.randn(len(x_linspace_est), config[\"latent_dim\"], device=device)\n",
        "        with torch.no_grad():\n",
        "            gamma, beta, _ = model.quantile_parameter(randn)\n",
        "            x_tmp = torch.from_numpy(x_linspace_est[:, None]).to(device).float()\n",
        "            alpha_tilde = model._quantile_inverse(x_tmp, gamma, beta, j)\n",
        "            alpha_est += alpha_tilde\n",
        "    alpha_est /= MC\n",
        "\n",
        "    # Convert to original scale for visualization\n",
        "    x_linspace_est = x_linspace_est * dataset.std[j] + dataset.mean[j]\n",
        "\n",
        "    # Calibration Step 1: Estimate F(x + 0.5), F(x - 0.5)\n",
        "    x_linspace = [np.arange(x, y+2, 1) - 0.5 for x, y in zip(\n",
        "        [int(np.min(df.to_numpy()[:, j]))],\n",
        "        [int(np.max(df.to_numpy()[:, j]))])][0]\n",
        "\n",
        "    print(\"Calibration Step 1...\")\n",
        "    alpha_hat = torch.zeros((len(x_linspace), 1), device=device)\n",
        "    for _ in tqdm.tqdm(range(MC), desc=\"Estimate CDF...\"):\n",
        "        randn = torch.randn(len(x_linspace), config[\"latent_dim\"], device=device)\n",
        "        with torch.no_grad():\n",
        "            gamma, beta, _ = model.quantile_parameter(randn)\n",
        "            x_tmp = torch.from_numpy(x_linspace[:, None]).to(device).float()\n",
        "            x_tmp = (x_tmp - dataset.mean[j]) / dataset.std[j]\n",
        "            alpha_tilde = model._quantile_inverse(x_tmp, gamma, beta, j)\n",
        "            alpha_hat += alpha_tilde\n",
        "    alpha_hat /= MC\n",
        "\n",
        "    # Move to CPU for numpy operations\n",
        "    alpha_hat_cpu = alpha_hat.cpu().numpy()\n",
        "\n",
        "    # Prepare x_linspace for steps 2 and 3\n",
        "    x_linspace_step2 = [np.arange(x, y+1, 1) for x, y in zip(\n",
        "        [int(np.min(df.to_numpy()[:, j]))],\n",
        "        [int(np.max(df.to_numpy()[:, j]))])][0]\n",
        "\n",
        "    # Calibration Step 2: Discretization F^*(x) = F^*(x-1) + F(x+0.5) - F(x-0.5)\n",
        "    print(\"Calibration Step 2...\")\n",
        "    alpha_cal = []\n",
        "    for i in range(len(alpha_hat_cpu)-1):\n",
        "        alpha_cal.append((alpha_hat_cpu[i+1] - alpha_hat_cpu[i]).item())\n",
        "    alpha_cal = np.array(alpha_cal) / np.sum(alpha_cal)\n",
        "    alpha_cal = np.cumsum(alpha_cal)\n",
        "\n",
        "    # Calibration Step 3: Ensure monotonicity\n",
        "    print(\"Calibration Step 3...\")\n",
        "    alpha_mono = [alpha_cal[0]]\n",
        "    for i in range(1, len(alpha_cal)):\n",
        "        if alpha_cal[i] < alpha_mono[-1]:\n",
        "            alpha_mono.append(alpha_mono[-1])\n",
        "        else:\n",
        "            alpha_mono.append(alpha_cal[i])\n",
        "\n",
        "    # Visualize the results\n",
        "    print(\"Visualizing results...\")\n",
        "    ecdf = ECDF(df.to_numpy()[:, j])\n",
        "    emp = [ecdf(x) for x in x_linspace_step2]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
        "\n",
        "    ax.step(x_linspace_step2, emp, label=\"empirical\", where='post',\n",
        "            linewidth=3.5, color=u'#ff7f0e')\n",
        "    ax.plot(x_linspace_est, alpha_est.cpu().numpy(), label=\"estimate\",\n",
        "            linewidth=3.5, color=u'#2ca02c')\n",
        "    ax.step(x_linspace_step2, alpha_mono, label=\"calibration\", where='post',\n",
        "            linewidth=3.5, linestyle='--', color='black')\n",
        "\n",
        "    ax.set_xlabel(dataset.continuous[j], fontsize=15)\n",
        "    ax.tick_params(axis='x', labelsize=14)\n",
        "    ax.tick_params(axis='y', labelsize=14)\n",
        "    plt.grid(True, axis='y', linestyle='--')\n",
        "\n",
        "    plt.legend(fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'./assets/{config[\"dataset\"]}/{config[\"dataset\"]}_CDF_calibration.png')\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"CDF calibration visualization saved to ./assets/{config['dataset']}/{config['dataset']}_CDF_calibration.png\")\n",
        "    print(\"Calibration complete!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"Created simple_calibration.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhhLTZPryZDn",
        "outputId": "6460aec8-45fc-4944-af26-4cc0c16e49f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created simple_calibration.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in ['adult', 'covtype', 'credit', 'loan', 'cabs', 'kings']:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Running calibration for dataset: {dataset}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    !python simple_calibration.py --dataset {dataset}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GflxLVztyny7",
        "outputId": "f16d0c96-59c3-4555-e10b-37399da21b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Running calibration for dataset: adult\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Calibrating dataset: adult\n",
            "/content/modules/adult_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_adult.pth\n",
            "Calibrating educational-num\n",
            "Estimating CDF...\n",
            "Estimate CDF...: 100% 1000/1000 [00:01<00:00, 770.23it/s]\n",
            "/content/simple_calibration.py:106: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_linspace_est = x_linspace_est * dataset.std[j] + dataset.mean[j]\n",
            "Calibration Step 1...\n",
            "Estimate CDF...:   0% 0/1000 [00:00<?, ?it/s]/content/simple_calibration.py:120: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_tmp = (x_tmp - dataset.mean[j]) / dataset.std[j]\n",
            "Estimate CDF...: 100% 1000/1000 [00:01<00:00, 705.01it/s]\n",
            "Calibration Step 2...\n",
            "Calibration Step 3...\n",
            "Visualizing results...\n",
            "CDF calibration visualization saved to ./assets/adult/adult_CDF_calibration.png\n",
            "Calibration complete!\n",
            "\n",
            "==================================================\n",
            "Running calibration for dataset: covtype\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Calibrating dataset: covtype\n",
            "/content/modules/covtype_datasets.py:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_covtype.pth\n",
            "Calibrating Aspect\n",
            "Estimating CDF...\n",
            "Estimate CDF...: 100% 1000/1000 [00:01<00:00, 536.89it/s]\n",
            "/content/simple_calibration.py:106: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_linspace_est = x_linspace_est * dataset.std[j] + dataset.mean[j]\n",
            "Calibration Step 1...\n",
            "Estimate CDF...:   0% 0/1000 [00:00<?, ?it/s]/content/simple_calibration.py:120: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_tmp = (x_tmp - dataset.mean[j]) / dataset.std[j]\n",
            "Estimate CDF...: 100% 1000/1000 [00:01<00:00, 524.40it/s]\n",
            "Calibration Step 2...\n",
            "Calibration Step 3...\n",
            "Visualizing results...\n",
            "CDF calibration visualization saved to ./assets/covtype/covtype_CDF_calibration.png\n",
            "Calibration complete!\n",
            "\n",
            "==================================================\n",
            "Running calibration for dataset: credit\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Calibrating dataset: credit\n",
            "Loading model from ./assets/DistVAE_credit.pth\n",
            "Calibrating AMT_CREDIT\n",
            "Estimating CDF...\n",
            "Estimate CDF...: 100% 1000/1000 [00:01<00:00, 518.53it/s]\n",
            "/content/simple_calibration.py:106: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_linspace_est = x_linspace_est * dataset.std[j] + dataset.mean[j]\n",
            "Calibration Step 1...\n",
            "Estimate CDF...:   0% 0/1000 [00:00<?, ?it/s]/content/simple_calibration.py:120: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_tmp = (x_tmp - dataset.mean[j]) / dataset.std[j]\n",
            "Estimate CDF...: 100% 1000/1000 [03:23<00:00,  4.92it/s]\n",
            "Calibration Step 2...\n",
            "Calibration Step 3...\n",
            "Visualizing results...\n",
            "/content/simple_calibration.py:170: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
            "  plt.tight_layout()\n",
            "/content/simple_calibration.py:171: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
            "  plt.savefig(f'./assets/{config[\"dataset\"]}/{config[\"dataset\"]}_CDF_calibration.png')\n",
            "CDF calibration visualization saved to ./assets/credit/credit_CDF_calibration.png\n",
            "Calibration complete!\n",
            "\n",
            "==================================================\n",
            "Running calibration for dataset: loan\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Calibrating dataset: loan\n",
            "/content/modules/loan_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_loan.pth\n",
            "Calibrating Experience\n",
            "Estimating CDF...\n",
            "Estimate CDF...: 100% 1000/1000 [00:01<00:00, 729.61it/s]\n",
            "/content/simple_calibration.py:106: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_linspace_est = x_linspace_est * dataset.std[j] + dataset.mean[j]\n",
            "Calibration Step 1...\n",
            "Estimate CDF...:   0% 0/1000 [00:00<?, ?it/s]/content/simple_calibration.py:120: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_tmp = (x_tmp - dataset.mean[j]) / dataset.std[j]\n",
            "Estimate CDF...: 100% 1000/1000 [00:01<00:00, 658.92it/s]\n",
            "Calibration Step 2...\n",
            "Calibration Step 3...\n",
            "Visualizing results...\n",
            "CDF calibration visualization saved to ./assets/loan/loan_CDF_calibration.png\n",
            "Calibration complete!\n",
            "\n",
            "==================================================\n",
            "Running calibration for dataset: cabs\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Calibrating dataset: cabs\n",
            "/content/modules/cabs_datasets.py:78: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:79: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_cabs.pth\n",
            "Calibrating Life_Style_Index\n",
            "Estimating CDF...\n",
            "Estimate CDF...: 100% 1000/1000 [00:01<00:00, 595.21it/s]\n",
            "/content/simple_calibration.py:106: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_linspace_est = x_linspace_est * dataset.std[j] + dataset.mean[j]\n",
            "Calibration Step 1...\n",
            "Estimate CDF...:   0% 0/1000 [00:00<?, ?it/s]/content/simple_calibration.py:120: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_tmp = (x_tmp - dataset.mean[j]) / dataset.std[j]\n",
            "Estimate CDF...: 100% 1000/1000 [00:01<00:00, 581.90it/s]\n",
            "Calibration Step 2...\n",
            "Calibration Step 3...\n",
            "Visualizing results...\n",
            "CDF calibration visualization saved to ./assets/cabs/cabs_CDF_calibration.png\n",
            "Calibration complete!\n",
            "\n",
            "==================================================\n",
            "Running calibration for dataset: kings\n",
            "==================================================\n",
            "Using device: cuda:0\n",
            "Calibrating dataset: kings\n",
            "/content/modules/kings_datasets.py:83: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_kings.pth\n",
            "Calibrating sqft_living\n",
            "Estimating CDF...\n",
            "Estimate CDF...: 100% 1000/1000 [00:01<00:00, 525.60it/s]\n",
            "/content/simple_calibration.py:106: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_linspace_est = x_linspace_est * dataset.std[j] + dataset.mean[j]\n",
            "Calibration Step 1...\n",
            "Estimate CDF...:   0% 0/1000 [00:00<?, ?it/s]/content/simple_calibration.py:120: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  x_tmp = (x_tmp - dataset.mean[j]) / dataset.std[j]\n",
            "Estimate CDF...: 100% 1000/1000 [00:02<00:00, 478.46it/s]\n",
            "Calibration Step 2...\n",
            "Calibration Step 3...\n",
            "Visualizing results...\n",
            "CDF calibration visualization saved to ./assets/kings/kings_CDF_calibration.png\n",
            "Calibration complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SYNTHEIZING**"
      ],
      "metadata": {
        "id": "ZecoVDzm89TI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create simple_synthesize.py\n",
        "with open('simple_synthesize.py', 'w') as f:\n",
        "    f.write('''\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from modules.simulation import set_random_seed\n",
        "from modules.model import VAE\n",
        "from modules.evaluation import (\n",
        "    regression_eval,\n",
        "    classification_eval,\n",
        "    statistical_similarity,\n",
        "    DCR_metric,\n",
        "    attribute_disclosure\n",
        ")\n",
        "\n",
        "# Configuration\n",
        "import argparse\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser('parameters')\n",
        "\n",
        "    parser.add_argument('--dataset', type=str, default='adult',\n",
        "                        help='Dataset options: covtype, credit, loan, adult, cabs, kings')\n",
        "    parser.add_argument('--beta', default=0.5, type=float,\n",
        "                        help='observation noise')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    # Your evaluation code will go here\n",
        "    args = get_args()\n",
        "    print(f\"Evaluating synthetic data quality for {args.dataset} dataset...\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"Created simple_synthesize.py file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN0Z-U1e886i",
        "outputId": "d75a161d-bab2-44ce-a23d-7b2029223cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created simple_synthesize.py file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREATE EVALUATION**"
      ],
      "metadata": {
        "id": "vt6j1nst_F4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create modules/evaluation.py\n",
        "with open('modules/evaluation.py', 'w') as f:\n",
        "    f.write('''\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import (\n",
        "    LinearRegression,\n",
        "    LogisticRegression\n",
        ")\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestRegressor,\n",
        "    RandomForestClassifier,\n",
        "    GradientBoostingRegressor,\n",
        "    GradientBoostingClassifier\n",
        ")\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "from scipy.spatial import distance_matrix\n",
        "\n",
        "from statsmodels.distributions.empirical_distribution import ECDF\n",
        "from scipy.stats import wasserstein_distance\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "def regression_eval(train, test, target, mean, std):\n",
        "    train[target] = train[target] * std + mean\n",
        "    test[target] = test[target] * std + mean\n",
        "\n",
        "    covariates = [x for x in train.columns if x not in [target]]\n",
        "\n",
        "    result = []\n",
        "    for name, regr in [\n",
        "        ('linear', None),\n",
        "        ('RF', RandomForestRegressor(random_state=0)),\n",
        "        ('GradBoost', GradientBoostingRegressor(random_state=0))]:\n",
        "\n",
        "        if name == 'linear':\n",
        "            regr = sm.OLS(train[target], train[covariates]).fit()\n",
        "        else:\n",
        "            regr.fit(train[covariates], train[target])\n",
        "        pred = regr.predict(test[covariates])\n",
        "\n",
        "        mare = (test[target] - pred).abs()\n",
        "        mare /= test[target].abs() + 1e-6\n",
        "        mare = mare.mean()\n",
        "\n",
        "        result.append((name, mare))\n",
        "        print(\"[{}] MARE: {:.3f}\".format(name, mare))\n",
        "    return result\n",
        "\n",
        "def classification_eval(train, test, target):\n",
        "    covariates = [x for x in train.columns if not x.startswith(target)]\n",
        "    target_ = [x for x in train.columns if x.startswith(target)]\n",
        "    train_target = train[target_].idxmax(axis=1)\n",
        "    test_target = test[target_].idxmax(axis=1).to_numpy()\n",
        "\n",
        "    result = []\n",
        "    for name, clf in [\n",
        "        ('logistic', LogisticRegression(multi_class='ovr', fit_intercept=False, max_iter=1000)),\n",
        "        ('RF', RandomForestClassifier(random_state=0)),\n",
        "        ('GradBoost', GradientBoostingClassifier(random_state=0))]:\n",
        "\n",
        "        clf.fit(train[covariates], train_target)\n",
        "        pred = clf.predict(test[covariates])\n",
        "\n",
        "        f1 = f1_score(test_target, pred, average='micro')\n",
        "\n",
        "        result.append((name, f1))\n",
        "        print(\"[{}] F1: {:.3f}\".format(name, f1))\n",
        "    return result\n",
        "\n",
        "def statistical_similarity(train, synthetic, standardize, continuous=None):\n",
        "    if standardize:\n",
        "        train[continuous] -= train[continuous].mean(axis=0)\n",
        "        train[continuous] /= train[continuous].std(axis=0)\n",
        "        train = train.to_numpy()\n",
        "\n",
        "        synthetic[continuous] -= synthetic[continuous].mean(axis=0)\n",
        "        synthetic[continuous] /= synthetic[continuous].std(axis=0)\n",
        "        synthetic = synthetic.to_numpy()\n",
        "\n",
        "    Dn_list = []\n",
        "    W1_list = []\n",
        "    for j in range(train.shape[1]):\n",
        "        xj = train[:, j]\n",
        "        ecdf = ECDF(xj)\n",
        "        ecdf_hat = ECDF(synthetic[:, j])\n",
        "\n",
        "        Dn = np.abs(ecdf(xj) - ecdf_hat(xj)).max()\n",
        "        W1 = wasserstein_distance(xj, synthetic[:, j])\n",
        "\n",
        "        Dn_list.append(Dn)\n",
        "        W1_list.append(W1)\n",
        "    return Dn_list, W1_list\n",
        "\n",
        "def DCR_metric(train, synthetic, data_percent=15):\n",
        "    \"\"\"\n",
        "    Computes the Distance to Closest Record metric.\n",
        "\n",
        "    Returns:\n",
        "    - 5th percentile distance between real and synthetic\n",
        "    - 5th percentile distance within real\n",
        "    - 5th percentile distance within synthetic\n",
        "    \"\"\"\n",
        "    # Sampling smaller sets to reduce computation time\n",
        "    real_sampled = train.sample(n=int(len(train)*(.01*data_percent)), random_state=42).to_numpy()\n",
        "    fake_sampled = synthetic.sample(n=int(len(synthetic)*(.01*data_percent)), random_state=42).to_numpy()\n",
        "\n",
        "    # Computing pair-wise distances\n",
        "    dist_rf = metrics.pairwise_distances(real_sampled, Y=fake_sampled, metric='minkowski', n_jobs=-1)\n",
        "    dist_rr = metrics.pairwise_distances(real_sampled, Y=None, metric='minkowski', n_jobs=-1)\n",
        "    dist_ff = metrics.pairwise_distances(fake_sampled, Y=None, metric='minkowski', n_jobs=-1)\n",
        "\n",
        "    # Removes distances of data points to themselves\n",
        "    rd_dist_rr = dist_rr[~np.eye(dist_rr.shape[0],dtype=bool)].reshape(dist_rr.shape[0],-1)\n",
        "    rd_dist_ff = dist_ff[~np.eye(dist_ff.shape[0],dtype=bool)].reshape(dist_ff.shape[0],-1)\n",
        "\n",
        "    # Computing smallest nearest neighbour distances\n",
        "    smallest_two_indexes_rf = [dist_rf[i].argsort()[:2] for i in range(len(dist_rf))]\n",
        "    smallest_two_rf = [dist_rf[i][smallest_two_indexes_rf[i]] for i in range(len(dist_rf))]\n",
        "    smallest_two_indexes_rr = [rd_dist_rr[i].argsort()[:2] for i in range(len(rd_dist_rr))]\n",
        "    smallest_two_rr = [rd_dist_rr[i][smallest_two_indexes_rr[i]] for i in range(len(rd_dist_rr))]\n",
        "    smallest_two_indexes_ff = [rd_dist_ff[i].argsort()[:2] for i in range(len(rd_dist_ff))]\n",
        "    smallest_two_ff = [rd_dist_ff[i][smallest_two_indexes_ff[i]] for i in range(len(rd_dist_ff))]\n",
        "\n",
        "    # Computing 5th percentiles\n",
        "    min_dist_rf = np.array([i[0] for i in smallest_two_rf])\n",
        "    fifth_perc_rf = np.percentile(min_dist_rf,5)\n",
        "    min_dist_rr = np.array([i[0] for i in smallest_two_rr])\n",
        "    fifth_perc_rr = np.percentile(min_dist_rr,5)\n",
        "    min_dist_ff = np.array([i[0] for i in smallest_two_ff])\n",
        "    fifth_perc_ff = np.percentile(min_dist_ff,5)\n",
        "\n",
        "    return [fifth_perc_rf, fifth_perc_rr, fifth_perc_ff]\n",
        "\n",
        "def attribute_disclosure(K, compromised, synthetic, attr_compromised, dataset):\n",
        "    dist = distance_matrix(\n",
        "        compromised[attr_compromised].to_numpy(),\n",
        "        synthetic[attr_compromised].to_numpy(),\n",
        "        p=2)\n",
        "    K_idx = dist.argsort(axis=1)[:, :K]\n",
        "\n",
        "    def most_common(lst):\n",
        "        return max(set(lst), key=lst.count)\n",
        "\n",
        "    votes = []\n",
        "    trues = []\n",
        "    for i in tqdm.tqdm(range(len(K_idx)), desc=\"Majority vote...\"):\n",
        "        true = np.zeros((len(dataset.discrete), ))\n",
        "        vote = np.zeros((len(dataset.discrete), ))\n",
        "        for j in range(len(dataset.discrete)):\n",
        "            true[j] = compromised.to_numpy()[i, len(dataset.continuous) + j]\n",
        "            vote[j] = most_common(list(synthetic.to_numpy()[K_idx[i], len(dataset.continuous) + j]))\n",
        "        votes.append(vote)\n",
        "        trues.append(true)\n",
        "    votes = np.vstack(votes)\n",
        "    trues = np.vstack(trues)\n",
        "\n",
        "    acc = 0\n",
        "    f1 = 0\n",
        "    for j in range(trues.shape[1]):\n",
        "        acc += (trues[:, j] == votes[:, j]).mean()\n",
        "        f1 += f1_score(trues[:, j], votes[:, j], average=\"macro\", zero_division=0)\n",
        "    acc /= trues.shape[1]\n",
        "    f1 /= trues.shape[1]\n",
        "\n",
        "    return acc, f1\n",
        "''')\n",
        "\n",
        "print(\"Created evaluation.py file\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dThGNk3P-XtE",
        "outputId": "c6f03ea6-82bd-4c3b-dfe5-c90c7757bd91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created evaluation.py file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREATE SYNTHESIZE**"
      ],
      "metadata": {
        "id": "Jf7Ak_JA_T9K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create complete updated simple_synthesize.py\n",
        "with open('simple_synthesize.py', 'w') as f:\n",
        "    f.write('''\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from modules.simulation import set_random_seed\n",
        "from modules.model import VAE\n",
        "from modules.evaluation import (\n",
        "    regression_eval,\n",
        "    classification_eval,\n",
        "    statistical_similarity,\n",
        "    DCR_metric,\n",
        "    attribute_disclosure\n",
        ")\n",
        "\n",
        "# Configuration\n",
        "import argparse\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser('parameters')\n",
        "\n",
        "    parser.add_argument('--dataset', type=str, default='adult',\n",
        "                        help='Dataset options: covtype, credit, loan, adult, cabs, kings')\n",
        "    parser.add_argument('--beta', default=0.5, type=float,\n",
        "                        help='observation noise')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    # Get configuration\n",
        "    args = get_args()\n",
        "    config = vars(args)\n",
        "\n",
        "    print(f\"Evaluating synthetic data quality for {config['dataset']} dataset...\")\n",
        "\n",
        "    # Create results storage\n",
        "    results = {\n",
        "        'dataset': config['dataset'],\n",
        "        'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    }\n",
        "\n",
        "    # Load dataset module\n",
        "    import importlib\n",
        "    dataset_module = importlib.import_module(f'modules.{config[\"dataset\"]}_datasets')\n",
        "    TabularDataset = dataset_module.TabularDataset\n",
        "\n",
        "    # Load real dataset\n",
        "    dataset = TabularDataset()\n",
        "    test_dataset = TabularDataset(train=False)\n",
        "\n",
        "    # Load synthetic data\n",
        "    syndata_path = f'./assets/synthetic_{config[\"dataset\"]}.csv'\n",
        "    print(f\"Loading synthetic data from {syndata_path}\")\n",
        "    syndata_original = pd.read_csv(syndata_path)\n",
        "\n",
        "    # Create a copy of synthetic data for preprocessing\n",
        "    syndata = syndata_original.copy()\n",
        "\n",
        "    # Convert categorical variables from strings to numeric codes\n",
        "    for i, dis in enumerate(dataset.discrete):\n",
        "        # Create a mapping from string values to numeric codes using the original dictionary\n",
        "        if dis in syndata.columns:\n",
        "            # Check if the column contains string values\n",
        "            if syndata[dis].dtype == 'object':\n",
        "                mapping = dataset.discrete_dicts[i]\n",
        "                # Apply the mapping, handle any new values by assigning a default value\n",
        "                syndata[dis] = syndata[dis].apply(lambda x: mapping.get(x, 0) if x in mapping else 0)\n",
        "\n",
        "    # Correlation Structure\n",
        "    try:\n",
        "        from dython.nominal import associations\n",
        "        print(\"\\\\nEvaluating Correlation Structure...\")\n",
        "        syn_asso = associations(\n",
        "            syndata_original, nominal_columns=dataset.discrete,\n",
        "            compute_only=True)\n",
        "        true_asso = associations(\n",
        "            dataset.train_raw, nominal_columns=dataset.discrete,\n",
        "            compute_only=True)\n",
        "        corr_dist = np.linalg.norm(true_asso[\"corr\"] - syn_asso[\"corr\"])\n",
        "        print(f'Correlation Matrix Distance: {corr_dist:.3f}')\n",
        "        results['corr_dist'] = corr_dist\n",
        "    except Exception as e:\n",
        "        print(f\"Error in correlation analysis: {e}\")\n",
        "        results['corr_dist'] = np.nan\n",
        "\n",
        "    # Statistical Similarity\n",
        "    print(\"\\\\nEvaluating Statistical Similarity...\")\n",
        "    try:\n",
        "        # Make sure all data is numeric for statistical similarity\n",
        "        train_copy = dataset.train_raw.copy()\n",
        "        syn_copy = syndata.copy()\n",
        "\n",
        "        Dn, W1 = statistical_similarity(\n",
        "            train_copy, syn_copy,\n",
        "            standardize=True, continuous=dataset.continuous)\n",
        "\n",
        "        cont_Dn = np.mean(Dn[:len(dataset.continuous)])\n",
        "        disc_Dn = np.mean(Dn[len(dataset.continuous):])\n",
        "        cont_W1 = np.mean(W1[:len(dataset.continuous)])\n",
        "        disc_W1 = np.mean(W1[len(dataset.continuous):])\n",
        "\n",
        "        print(f'K-S (continuous): {cont_Dn:.3f}')\n",
        "        print(f'1-WD (continuous): {cont_W1:.3f}')\n",
        "        print(f'K-S (discrete): {disc_Dn:.3f}')\n",
        "        print(f'1-WD (discrete): {disc_W1:.3f}')\n",
        "\n",
        "        results['K-S_continuous'] = cont_Dn\n",
        "        results['1-WD_continuous'] = cont_W1\n",
        "        results['K-S_discrete'] = disc_Dn\n",
        "        results['1-WD_discrete'] = disc_W1\n",
        "    except Exception as e:\n",
        "        print(f\"Error in statistical similarity analysis: {e}\")\n",
        "        results['K-S_continuous'] = np.nan\n",
        "        results['1-WD_continuous'] = np.nan\n",
        "        results['K-S_discrete'] = np.nan\n",
        "        results['1-WD_discrete'] = np.nan\n",
        "\n",
        "    # Distance to Closest Record\n",
        "    print(\"\\\\nEvaluating Distance to Closest Record...\")\n",
        "    try:\n",
        "        # Standardize synthetic data - only use continuous columns\n",
        "        train_cont = dataset.train[dataset.continuous].copy()\n",
        "        syn_cont = syndata[dataset.continuous].copy()\n",
        "\n",
        "        # Ensure all data is numeric\n",
        "        train_cont = train_cont.apply(pd.to_numeric, errors='coerce')\n",
        "        syn_cont = syn_cont.apply(pd.to_numeric, errors='coerce')\n",
        "\n",
        "        # Standardize\n",
        "        syn_cont = (syn_cont - syn_cont.mean()) / syn_cont.std()\n",
        "\n",
        "        DCR = DCR_metric(train_cont, syn_cont)\n",
        "\n",
        "        print(f'DCR (R&S): {DCR[0]:.3f}')\n",
        "        print(f'DCR (R): {DCR[1]:.3f}')\n",
        "        print(f'DCR (S): {DCR[2]:.3f}')\n",
        "\n",
        "        results['DCR_R&S'] = DCR[0]\n",
        "        results['DCR_R'] = DCR[1]\n",
        "        results['DCR_S'] = DCR[2]\n",
        "    except Exception as e:\n",
        "        print(f\"Error in DCR analysis: {e}\")\n",
        "        results['DCR_R&S'] = np.nan\n",
        "        results['DCR_R'] = np.nan\n",
        "        results['DCR_S'] = np.nan\n",
        "\n",
        "    # Attribute Disclosure\n",
        "    print(\"\\\\nEvaluating Attribute Disclosure...\")\n",
        "    try:\n",
        "        # Create a standardized version of train_raw\n",
        "        train_raw_numeric = dataset.train_raw.copy()\n",
        "\n",
        "        # Sample a subset of records for compromise\n",
        "        compromised_idx = np.random.choice(\n",
        "            range(len(train_raw_numeric)),\n",
        "            min(400, int(len(train_raw_numeric) * 0.01)),\n",
        "            replace=False)\n",
        "\n",
        "        # Standardize continuous columns\n",
        "        train_raw_numeric[dataset.continuous] = (train_raw_numeric[dataset.continuous] -\n",
        "                                            train_raw_numeric[dataset.continuous].mean()) / train_raw_numeric[dataset.continuous].std()\n",
        "\n",
        "        # Get compromised records\n",
        "        compromised = train_raw_numeric.iloc[compromised_idx].reset_index(drop=True)\n",
        "\n",
        "        # Check for attribute disclosure with different K values\n",
        "        attr_num = min(5, len(dataset.continuous))\n",
        "        attr_compromised = dataset.continuous[:attr_num]\n",
        "\n",
        "        for K in [1, 10, 100]:\n",
        "            try:\n",
        "                acc, f1 = attribute_disclosure(\n",
        "                    K, compromised, syndata, attr_compromised, dataset)\n",
        "                print(f'AD F1 (S={attr_num},K={K}): {f1:.3f}')\n",
        "                results[f'AD_F1_K{K}'] = f1\n",
        "            except Exception as e:\n",
        "                print(f\"Error in attribute disclosure evaluation for K={K}: {e}\")\n",
        "                results[f'AD_F1_K{K}'] = np.nan\n",
        "    except Exception as e:\n",
        "        print(f\"Error in attribute disclosure setup: {e}\")\n",
        "        results['AD_F1_K1'] = np.nan\n",
        "        results['AD_F1_K10'] = np.nan\n",
        "        results['AD_F1_K100'] = np.nan\n",
        "\n",
        "    # ML Utility - Regression\n",
        "    print(\"\\\\nEvaluating Machine Learning Utility in Regression...\")\n",
        "    try:\n",
        "        # For the real data baseline\n",
        "        base_reg = regression_eval(\n",
        "            dataset.train.copy(), test_dataset.test.copy(), dataset.RegTarget,\n",
        "            dataset.mean[dataset.RegTarget], dataset.std[dataset.RegTarget])\n",
        "        base_reg_score = np.mean([x[1] for x in base_reg])\n",
        "        print(f'MARE (Baseline): {base_reg_score:.3f}')\n",
        "        results['MARE_baseline'] = base_reg_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in baseline regression evaluation: {e}\")\n",
        "        results['MARE_baseline'] = np.nan\n",
        "\n",
        "    try:\n",
        "        # Prepare synthetic data for ML evaluation\n",
        "        syn_ml = pd.DataFrame()\n",
        "\n",
        "        # Add continuous columns\n",
        "        for col in dataset.continuous:\n",
        "            if col in syndata.columns:\n",
        "                syn_ml[col] = pd.to_numeric(syndata[col], errors='coerce')\n",
        "\n",
        "        # Standardize continuous columns\n",
        "        mean_vals = syn_ml[dataset.continuous].mean()\n",
        "        std_vals = syn_ml[dataset.continuous].std()\n",
        "        syn_ml[dataset.continuous] = (syn_ml[dataset.continuous] - mean_vals) / std_vals\n",
        "\n",
        "        # Add dummy columns for discrete variables\n",
        "        for i, dis in enumerate(dataset.discrete):\n",
        "            # Get original dummy columns from training data\n",
        "            original_dummies = [c for c in dataset.train.columns if c.startswith(f\"{dis}_\")]\n",
        "\n",
        "            # Create numeric version of synthetic discrete column\n",
        "            if dis in syndata.columns:\n",
        "                syn_discrete = syndata[dis].copy()\n",
        "                # Convert to numeric using mapping if necessary\n",
        "                if syn_discrete.dtype == 'object':\n",
        "                    syn_discrete = syn_discrete.map(dataset.discrete_dicts[i]).fillna(0).astype(int)\n",
        "                # Create dummies\n",
        "                syn_dummies = pd.get_dummies(syn_discrete, prefix=dis)\n",
        "\n",
        "                # Ensure all original dummy columns exist\n",
        "                for dummy_col in original_dummies:\n",
        "                    if dummy_col not in syn_dummies.columns:\n",
        "                        syn_dummies[dummy_col] = 0\n",
        "\n",
        "                # Add to synthetic ML data\n",
        "                for dummy_col in original_dummies:\n",
        "                    if dummy_col in syn_dummies.columns:\n",
        "                        syn_ml[dummy_col] = syn_dummies[dummy_col]\n",
        "                    else:\n",
        "                        syn_ml[dummy_col] = 0\n",
        "\n",
        "        # Ensure all columns from original training data are present\n",
        "        for col in dataset.train.columns:\n",
        "            if col not in syn_ml.columns:\n",
        "                syn_ml[col] = 0\n",
        "\n",
        "        # Match column order to original training data\n",
        "        syn_ml = syn_ml[dataset.train.columns]\n",
        "\n",
        "        # Convert all to float type to avoid object type issues\n",
        "        syn_ml = syn_ml.astype(float)\n",
        "\n",
        "        # Evaluate regression performance with synthetic data\n",
        "        syn_reg = regression_eval(\n",
        "            syn_ml, test_dataset.test.copy(), dataset.RegTarget,\n",
        "            mean_vals[dataset.RegTarget], std_vals[dataset.RegTarget])\n",
        "        syn_reg_score = np.mean([x[1] for x in syn_reg])\n",
        "        print(f'MARE (Synthetic): {syn_reg_score:.3f}')\n",
        "        results['MARE_synthetic'] = syn_reg_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in synthetic data regression evaluation: {e}\")\n",
        "        results['MARE_synthetic'] = np.nan\n",
        "\n",
        "    # ML Utility - Classification\n",
        "    print(\"\\\\nEvaluating Machine Learning Utility in Classification...\")\n",
        "    try:\n",
        "        # Evaluate classification on real data\n",
        "        base_clf = classification_eval(\n",
        "            dataset.train.copy(), test_dataset.test.copy(), dataset.ClfTarget)\n",
        "        base_clf_score = np.mean([x[1] for x in base_clf])\n",
        "        print(f'F1 (Baseline): {base_clf_score:.3f}')\n",
        "        results['F1_baseline'] = base_clf_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in baseline classification evaluation: {e}\")\n",
        "        results['F1_baseline'] = np.nan\n",
        "\n",
        "    try:\n",
        "        # Evaluate classification on synthetic data\n",
        "        syn_clf = classification_eval(\n",
        "            syn_ml, test_dataset.test.copy(), dataset.ClfTarget)\n",
        "        syn_clf_score = np.mean([x[1] for x in syn_clf])\n",
        "        print(f'F1 (Synthetic): {syn_clf_score:.3f}')\n",
        "        results['F1_synthetic'] = syn_clf_score\n",
        "    except Exception as e:\n",
        "        print(f\"Error in synthetic data classification evaluation: {e}\")\n",
        "        results['F1_synthetic'] = np.nan\n",
        "\n",
        "    # Create results directory\n",
        "    os.makedirs('./assets/results/', exist_ok=True)\n",
        "\n",
        "    # Save the results to CSV\n",
        "    results_df = pd.DataFrame([results])\n",
        "    results_path = f'./assets/results/{config[\"dataset\"]}_evaluation.csv'\n",
        "    results_df.to_csv(results_path, index=False)\n",
        "\n",
        "    print(f\"\\\\nEvaluation complete! Results saved to {results_path}\")\n",
        "    print(\"\\\\nResults summary:\")\n",
        "    print(results_df)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"Created comprehensive updated simple_synthesize.py that properly saves results\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhD9LRnp_Peb",
        "outputId": "fb308e0f-7da6-498e-944f-adc87bc99571",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created comprehensive updated simple_synthesize.py that properly saves results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_synthesize.py --dataset adult"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcrAXfCy_bqP",
        "outputId": "0047fd08-9664-470b-bce4-21e5d1bcbbf8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating synthetic data quality for adult dataset...\n",
            "/content/modules/adult_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/adult_datasets.py:87: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:88: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading synthetic data from ./assets/synthetic_adult.csv\n",
            "\n",
            "Evaluating Correlation Structure...\n",
            "Correlation Matrix Distance: 1.157\n",
            "\n",
            "Evaluating Statistical Similarity...\n",
            "K-S (continuous): 0.142\n",
            "1-WD (continuous): 0.151\n",
            "K-S (discrete): 0.051\n",
            "1-WD (discrete): 0.299\n",
            "\n",
            "Evaluating Distance to Closest Record...\n",
            "DCR (R&S): 0.066\n",
            "DCR (R): 0.000\n",
            "DCR (S): 0.009\n",
            "\n",
            "Evaluating Attribute Disclosure...\n",
            "Majority vote...: 100% 400/400 [00:01<00:00, 249.11it/s]\n",
            "AD F1 (S=5,K=1): 0.149\n",
            "Majority vote...: 100% 400/400 [00:01<00:00, 228.13it/s]\n",
            "AD F1 (S=5,K=10): 0.147\n",
            "Majority vote...: 100% 400/400 [00:01<00:00, 227.33it/s]\n",
            "AD F1 (S=5,K=100): 0.134\n",
            "\n",
            "Evaluating Machine Learning Utility in Regression...\n",
            "Error in baseline regression evaluation: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).\n",
            "/content/simple_synthesize.py:239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  syn_ml[dummy_col] = syn_dummies[dummy_col]\n",
            "/content/simple_synthesize.py:239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  syn_ml[dummy_col] = syn_dummies[dummy_col]\n",
            "/content/simple_synthesize.py:239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  syn_ml[dummy_col] = syn_dummies[dummy_col]\n",
            "/content/simple_synthesize.py:239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  syn_ml[dummy_col] = syn_dummies[dummy_col]\n",
            "/content/simple_synthesize.py:239: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  syn_ml[dummy_col] = syn_dummies[dummy_col]\n",
            "[linear] MARE: 0.260\n",
            "[RF] MARE: 0.279\n",
            "[GradBoost] MARE: 0.262\n",
            "MARE (Synthetic): 0.267\n",
            "\n",
            "Evaluating Machine Learning Utility in Classification...\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[logistic] F1: 0.847\n",
            "[RF] F1: 0.845\n",
            "[GradBoost] F1: 0.861\n",
            "F1 (Baseline): 0.851\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[logistic] F1: 0.810\n",
            "[RF] F1: 0.763\n",
            "[GradBoost] F1: 0.765\n",
            "F1 (Synthetic): 0.779\n",
            "\n",
            "Evaluation complete! Results saved to ./assets/results/adult_evaluation.csv\n",
            "\n",
            "Results summary:\n",
            "  dataset            timestamp  ...  F1_baseline  F1_synthetic\n",
            "0   adult  2025-04-24 15:10:11  ...     0.851143      0.779203\n",
            "\n",
            "[1 rows x 17 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate synthetic data for covtype dataset\n",
        "!python simple_synthesize.py --dataset covtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SHP3YDArBLpt",
        "outputId": "70cc81b7-3106-41f0-8e1a-7d22d7fa4cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating synthetic data quality for covtype dataset...\n",
            "/content/modules/covtype_datasets.py:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/covtype_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:85: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading synthetic data from ./assets/synthetic_covtype.csv\n",
            "\n",
            "Evaluating Correlation Structure...\n",
            "Correlation Matrix Distance: 1.641\n",
            "\n",
            "Evaluating Statistical Similarity...\n",
            "K-S (continuous): 0.044\n",
            "1-WD (continuous): 0.064\n",
            "K-S (discrete): 0.488\n",
            "1-WD (discrete): 0.979\n",
            "\n",
            "Evaluating Distance to Closest Record...\n",
            "DCR (R&S): 0.810\n",
            "DCR (R): 0.329\n",
            "DCR (S): 0.865\n",
            "\n",
            "Evaluating Attribute Disclosure...\n",
            "Majority vote...: 100% 400/400 [00:00<00:00, 30529.01it/s]\n",
            "AD F1 (S=5,K=1): 0.000\n",
            "Majority vote...: 100% 400/400 [00:00<00:00, 30719.63it/s]\n",
            "AD F1 (S=5,K=10): 0.000\n",
            "Majority vote...: 100% 400/400 [00:00<00:00, 21237.25it/s]\n",
            "AD F1 (S=5,K=100): 0.000\n",
            "\n",
            "Evaluating Machine Learning Utility in Regression...\n",
            "Error in baseline regression evaluation: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).\n",
            "[linear] MARE: 0.418\n",
            "[RF] MARE: 0.076\n",
            "[GradBoost] MARE: 0.076\n",
            "MARE (Synthetic): 0.190\n",
            "\n",
            "Evaluating Machine Learning Utility in Classification...\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[logistic] F1: 0.533\n",
            "[RF] F1: 0.868\n",
            "[GradBoost] F1: 0.751\n",
            "F1 (Baseline): 0.717\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[logistic] F1: 0.140\n",
            "[RF] F1: 0.006\n",
            "[GradBoost] F1: 0.007\n",
            "F1 (Synthetic): 0.051\n",
            "\n",
            "Evaluation complete! Results saved to ./assets/results/covtype_evaluation.csv\n",
            "\n",
            "Results summary:\n",
            "   dataset            timestamp  ...  F1_baseline  F1_synthetic\n",
            "0  covtype  2025-04-24 15:11:44  ...     0.717467         0.051\n",
            "\n",
            "[1 rows x 17 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_synthesize.py --dataset credit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "11RVKDk3Eyee",
        "outputId": "659fbc18-dd5f-4b26-b420-dd0ff8500d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating synthetic data quality for credit dataset...\n",
            "Loading synthetic data from ./assets/synthetic_credit.csv\n",
            "\n",
            "Evaluating Correlation Structure...\n",
            "Correlation Matrix Distance: 2.818\n",
            "\n",
            "Evaluating Statistical Similarity...\n",
            "K-S (continuous): 0.129\n",
            "1-WD (continuous): 0.128\n",
            "K-S (discrete): 0.022\n",
            "1-WD (discrete): 0.046\n",
            "\n",
            "Evaluating Distance to Closest Record...\n",
            "DCR (R&S): 0.841\n",
            "DCR (R): 0.558\n",
            "DCR (S): 0.901\n",
            "\n",
            "Evaluating Attribute Disclosure...\n",
            "Majority vote...: 100% 169/169 [00:00<00:00, 310.61it/s]\n",
            "AD F1 (S=5,K=1): 0.278\n",
            "Majority vote...: 100% 169/169 [00:00<00:00, 312.96it/s]\n",
            "AD F1 (S=5,K=10): 0.291\n",
            "Majority vote...: 100% 169/169 [00:00<00:00, 315.51it/s]\n",
            "AD F1 (S=5,K=100): 0.282\n",
            "\n",
            "Evaluating Machine Learning Utility in Regression...\n",
            "Error in baseline regression evaluation: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).\n",
            "[linear] MARE: nan\n",
            "Error in synthetic data regression evaluation: Found array with 0 sample(s) (shape=(0, 46)) while a minimum of 1 is required by RandomForestRegressor.\n",
            "\n",
            "Evaluating Machine Learning Utility in Classification...\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "Error in baseline classification evaluation: Found array with 0 sample(s) (shape=(0, 45)) while a minimum of 1 is required by LogisticRegression.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "Error in synthetic data classification evaluation: Found array with 0 sample(s) (shape=(0, 45)) while a minimum of 1 is required by LogisticRegression.\n",
            "\n",
            "Evaluation complete! Results saved to ./assets/results/credit_evaluation.csv\n",
            "\n",
            "Results summary:\n",
            "  dataset            timestamp  ...  F1_baseline  F1_synthetic\n",
            "0  credit  2025-04-24 15:15:27  ...          NaN           NaN\n",
            "\n",
            "[1 rows x 17 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_synthesize.py --dataset loan\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyOup87oJC_f",
        "outputId": "d92d2387-06d5-4021-b6e6-66c535c41ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating synthetic data quality for loan dataset...\n",
            "/content/modules/loan_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/loan_datasets.py:87: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:88: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading synthetic data from ./assets/synthetic_loan.csv\n",
            "\n",
            "Evaluating Correlation Structure...\n",
            "Correlation Matrix Distance: 2.106\n",
            "\n",
            "Evaluating Statistical Similarity...\n",
            "K-S (continuous): 0.128\n",
            "1-WD (continuous): 0.178\n",
            "K-S (discrete): 0.052\n",
            "1-WD (discrete): 0.161\n",
            "\n",
            "Evaluating Distance to Closest Record...\n",
            "DCR (R&S): 0.315\n",
            "DCR (R): 0.109\n",
            "DCR (S): 0.289\n",
            "\n",
            "Evaluating Attribute Disclosure...\n",
            "Majority vote...: 100% 40/40 [00:00<00:00, 1993.11it/s]\n",
            "AD F1 (S=5,K=1): 0.360\n",
            "Majority vote...: 100% 40/40 [00:00<00:00, 1845.58it/s]\n",
            "AD F1 (S=5,K=10): 0.392\n",
            "Majority vote...: 100% 40/40 [00:00<00:00, 1481.42it/s]\n",
            "AD F1 (S=5,K=100): 0.386\n",
            "\n",
            "Evaluating Machine Learning Utility in Regression...\n",
            "Error in baseline regression evaluation: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).\n",
            "[linear] MARE: 0.228\n",
            "[RF] MARE: 0.228\n",
            "[GradBoost] MARE: 0.216\n",
            "MARE (Synthetic): 0.224\n",
            "\n",
            "Evaluating Machine Learning Utility in Classification...\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[logistic] F1: 0.934\n",
            "[RF] F1: 0.959\n",
            "[GradBoost] F1: 0.950\n",
            "F1 (Baseline): 0.948\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[logistic] F1: 0.897\n",
            "[RF] F1: 0.895\n",
            "[GradBoost] F1: 0.861\n",
            "F1 (Synthetic): 0.884\n",
            "\n",
            "Evaluation complete! Results saved to ./assets/results/loan_evaluation.csv\n",
            "\n",
            "Results summary:\n",
            "  dataset            timestamp  ...  F1_baseline  F1_synthetic\n",
            "0    loan  2025-04-24 15:16:17  ...     0.947667      0.884333\n",
            "\n",
            "[1 rows x 17 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_synthesize.py --dataset cabs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yyc4EbPvJLKN",
        "outputId": "800075ff-e66d-4c63-f15c-f61f4bc685cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating synthetic data quality for cabs dataset...\n",
            "/content/modules/cabs_datasets.py:78: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:79: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/cabs_datasets.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:96: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading synthetic data from ./assets/synthetic_cabs.csv\n",
            "\n",
            "Evaluating Correlation Structure...\n",
            "Correlation Matrix Distance: 1.425\n",
            "\n",
            "Evaluating Statistical Similarity...\n",
            "K-S (continuous): 0.056\n",
            "1-WD (continuous): 0.078\n",
            "K-S (discrete): 0.071\n",
            "1-WD (discrete): 0.236\n",
            "\n",
            "Evaluating Distance to Closest Record...\n",
            "DCR (R&S): 0.401\n",
            "DCR (R): 0.361\n",
            "DCR (S): 0.397\n",
            "\n",
            "Evaluating Attribute Disclosure...\n",
            "Majority vote...: 100% 223/223 [00:00<00:00, 587.58it/s]\n",
            "AD F1 (S=5,K=1): 0.152\n",
            "Majority vote...: 100% 223/223 [00:00<00:00, 527.35it/s]\n",
            "AD F1 (S=5,K=10): 0.159\n",
            "Majority vote...: 100% 223/223 [00:00<00:00, 517.14it/s]\n",
            "AD F1 (S=5,K=100): 0.159\n",
            "\n",
            "Evaluating Machine Learning Utility in Regression...\n",
            "Error in baseline regression evaluation: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).\n",
            "[linear] MARE: 0.611\n",
            "[RF] MARE: 0.651\n",
            "[GradBoost] MARE: 0.620\n",
            "MARE (Synthetic): 0.627\n",
            "\n",
            "Evaluating Machine Learning Utility in Classification...\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[logistic] F1: 0.709\n",
            "[RF] F1: 0.706\n",
            "[GradBoost] F1: 0.708\n",
            "F1 (Baseline): 0.708\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[logistic] F1: 0.709\n",
            "[RF] F1: 0.677\n",
            "[GradBoost] F1: 0.464\n",
            "F1 (Synthetic): 0.617\n",
            "\n",
            "Evaluation complete! Results saved to ./assets/results/cabs_evaluation.csv\n",
            "\n",
            "Results summary:\n",
            "  dataset            timestamp  ...  F1_baseline  F1_synthetic\n",
            "0    cabs  2025-04-24 15:16:29  ...     0.708042      0.616813\n",
            "\n",
            "[1 rows x 17 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python simple_synthesize.py --dataset kings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yTKQ2cfJcbv",
        "outputId": "e797fd6b-70c0-41ad-e730-4b4cb7c3136a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating synthetic data quality for kings dataset...\n",
            "/content/modules/kings_datasets.py:83: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/kings_datasets.py:98: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:99: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading synthetic data from ./assets/synthetic_kings.csv\n",
            "\n",
            "Evaluating Correlation Structure...\n",
            "Correlation Matrix Distance: 1.779\n",
            "\n",
            "Evaluating Statistical Similarity...\n",
            "K-S (continuous): 0.146\n",
            "1-WD (continuous): 0.123\n",
            "K-S (discrete): 0.391\n",
            "1-WD (discrete): 1.295\n",
            "\n",
            "Evaluating Distance to Closest Record...\n",
            "DCR (R&S): 0.574\n",
            "DCR (R): 0.199\n",
            "DCR (S): 0.632\n",
            "\n",
            "Evaluating Attribute Disclosure...\n",
            "Majority vote...: 100% 200/200 [00:00<00:00, 405.87it/s]\n",
            "AD F1 (S=5,K=1): 0.121\n",
            "Majority vote...: 100% 200/200 [00:00<00:00, 380.17it/s]\n",
            "AD F1 (S=5,K=10): 0.128\n",
            "Majority vote...: 100% 200/200 [00:00<00:00, 374.44it/s]\n",
            "AD F1 (S=5,K=100): 0.128\n",
            "\n",
            "Evaluating Machine Learning Utility in Regression...\n",
            "Error in baseline regression evaluation: Pandas data cast to numpy dtype of object. Check input data with np.asarray(data).\n",
            "[linear] MARE: 0.001\n",
            "[RF] MARE: 0.001\n",
            "[GradBoost] MARE: 0.001\n",
            "MARE (Synthetic): 0.001\n",
            "\n",
            "Evaluating Machine Learning Utility in Classification...\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[logistic] F1: 0.668\n",
            "[RF] F1: 0.710\n",
            "[GradBoost] F1: 0.705\n",
            "F1 (Baseline): 0.694\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "[logistic] F1: 0.205\n",
            "[RF] F1: 0.252\n",
            "[GradBoost] F1: 0.219\n",
            "F1 (Synthetic): 0.226\n",
            "\n",
            "Evaluation complete! Results saved to ./assets/results/kings_evaluation.csv\n",
            "\n",
            "Results summary:\n",
            "  dataset            timestamp  ...  F1_baseline  F1_synthetic\n",
            "0   kings  2025-04-24 15:17:24  ...     0.694152      0.225666\n",
            "\n",
            "[1 rows x 17 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHADOW DATA SCRIPT**"
      ],
      "metadata": {
        "id": "wbMCO5xFSX3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix shadow_data.py by adding batch_size to config\n",
        "with open('shadow_data.py', 'w') as f:\n",
        "    f.write('''\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from modules.simulation import set_random_seed\n",
        "from modules.model import VAE\n",
        "\n",
        "# Configuration\n",
        "import argparse\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser('parameters')\n",
        "\n",
        "    parser.add_argument('--dataset', type=str, default='adult',\n",
        "                        help='Dataset options: covtype, credit, loan, adult, cabs, kings')\n",
        "    parser.add_argument('--seed', type=int, default=1,\n",
        "                        help='random seed')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "def main():\n",
        "    # Get configuration\n",
        "    args = get_args()\n",
        "    config = vars(args)\n",
        "\n",
        "    # Create privacy directory\n",
        "    if not os.path.exists(f'./privacy/{config[\"dataset\"]}'):\n",
        "        os.makedirs(f'./privacy/{config[\"dataset\"]}')\n",
        "\n",
        "    config[\"cuda\"] = torch.cuda.is_available()\n",
        "    device = torch.device('cuda:0') if config[\"cuda\"] else torch.device('cpu')\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Generating shadow data for dataset: {config['dataset']}\")\n",
        "\n",
        "    # Set random seed\n",
        "    set_random_seed(config[\"seed\"])\n",
        "    torch.manual_seed(config[\"seed\"])\n",
        "    if config[\"cuda\"]:\n",
        "        torch.cuda.manual_seed(config[\"seed\"])\n",
        "\n",
        "    # Import dataset module\n",
        "    import importlib\n",
        "    dataset_module = importlib.import_module(f'modules.{config[\"dataset\"]}_datasets')\n",
        "    TabularDataset = dataset_module.TabularDataset\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = TabularDataset()\n",
        "    test_dataset = TabularDataset(train=False)\n",
        "\n",
        "    # Load model\n",
        "    model_path = f'./assets/DistVAE_{config[\"dataset\"]}.pth'\n",
        "    print(f\"Loading model from {model_path}\")\n",
        "\n",
        "    # Update config with dataset dimensions and other required parameters\n",
        "    OutputInfo_list = dataset.OutputInfo_list\n",
        "    CRPS_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'CRPS'])\n",
        "    softmax_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'softmax'])\n",
        "    config[\"CRPS_dim\"] = CRPS_dim\n",
        "    config[\"softmax_dim\"] = softmax_dim\n",
        "    config[\"latent_dim\"] = 2\n",
        "    config[\"step\"] = 0.1\n",
        "    config[\"threshold\"] = 1e-5\n",
        "    config[\"beta\"] = 0.5\n",
        "    config[\"batch_size\"] = 256  # Added missing batch_size\n",
        "\n",
        "    # Initialize model\n",
        "    model = VAE(config, device).to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Generate shadow datasets\n",
        "    print(\"Generating shadow data...\")\n",
        "    n_train = len(dataset.train)\n",
        "    n_test = len(test_dataset.test)\n",
        "    K = 1  # Number of shadow models\n",
        "\n",
        "    for s in tqdm.tqdm(range(K), desc=\"Generating shadow train and test datasets...\"):\n",
        "        torch.manual_seed(s)\n",
        "\n",
        "        # Generate synthetic train data\n",
        "        with torch.no_grad():\n",
        "            train_synthetic = model.generate_data(n_train, OutputInfo_list, dataset)\n",
        "\n",
        "        train_path = f'./privacy/{config[\"dataset\"]}/train_{config[\"seed\"]}_synthetic{s}.csv'\n",
        "        train_synthetic.to_csv(train_path)\n",
        "        print(f\"Shadow training data saved to {train_path}\")\n",
        "\n",
        "        # Generate synthetic test data\n",
        "        with torch.no_grad():\n",
        "            test_synthetic = model.generate_data(n_test, OutputInfo_list, dataset)\n",
        "\n",
        "        test_path = f'./privacy/{config[\"dataset\"]}/test_{config[\"seed\"]}_synthetic{s}.csv'\n",
        "        test_synthetic.to_csv(test_path)\n",
        "        print(f\"Shadow test data saved to {test_path}\")\n",
        "\n",
        "    print(\"Shadow data generation complete!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"Fixed shadow_data.py by adding batch_size to config\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx6RdMBOSXbZ",
        "outputId": "e5906aae-e275-40d0-cbeb-3b5de528a045"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed shadow_data.py by adding batch_size to config\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHADOW MAIN**"
      ],
      "metadata": {
        "id": "PJXeqxatSgt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplify the shadow_main.py approach\n",
        "with open('shadow_main.py', 'w') as f:\n",
        "    f.write('''\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "\n",
        "from modules.simulation import set_random_seed\n",
        "from modules.model import VAE\n",
        "from modules.train import train_VAE\n",
        "\n",
        "# Configuration\n",
        "import argparse\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser('parameters')\n",
        "\n",
        "    parser.add_argument('--seed', type=int, default=1,\n",
        "                        help='seed for repeatable results')\n",
        "    parser.add_argument('--dataset', type=str, default='adult',\n",
        "                        help='Dataset options')\n",
        "\n",
        "    parser.add_argument(\"--latent_dim\", default=2, type=int,\n",
        "                        help=\"the latent dimension size\")\n",
        "    parser.add_argument(\"--step\", default=0.1, type=float,\n",
        "                        help=\"interval size of quantile levels\")\n",
        "\n",
        "    parser.add_argument('--epochs', default=5, type=int,\n",
        "                        help='the number of epochs')\n",
        "    parser.add_argument('--batch_size', default=256, type=int,\n",
        "                        help='batch size')\n",
        "    parser.add_argument('--lr', default=1e-3, type=float,\n",
        "                        help='learning rate')\n",
        "    parser.add_argument('--threshold', default=1e-5, type=float,\n",
        "                        help='threshold for clipping alpha_tilde')\n",
        "\n",
        "    parser.add_argument('--beta', default=0.5, type=float,\n",
        "                        help='scale parameter')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "class ShadowDataset(Dataset):\n",
        "    def __init__(self, file_path, dataset_template):\n",
        "        \"\"\"\n",
        "        Creates a shadow dataset based on the original dataset's structure\n",
        "        file_path: path to the shadow data CSV\n",
        "        dataset_template: original TabularDataset instance to copy preprocessing from\n",
        "        \"\"\"\n",
        "        self.template = dataset_template\n",
        "\n",
        "        # Load data\n",
        "        base = pd.read_csv(file_path)\n",
        "\n",
        "        # Process continuous and discrete columns like the original dataset\n",
        "        # Map discrete values to their indices\n",
        "        for i, dis in enumerate(self.template.discrete):\n",
        "            if dis in base.columns:\n",
        "                mapping = self.template.discrete_dicts[i]\n",
        "                base[dis] = base[dis].apply(lambda x: mapping.get(x, 0) if isinstance(x, str) else int(x))\n",
        "\n",
        "        # One-hot encoding\n",
        "        df_dummy = []\n",
        "        for d in self.template.discrete:\n",
        "            if d in base.columns:\n",
        "                df_dummy.append(pd.get_dummies(base[d], prefix=d))\n",
        "\n",
        "        # Combine continuous and one-hot encoded discrete\n",
        "        base_dummy = pd.concat([base[self.template.continuous]] + df_dummy, axis=1)\n",
        "\n",
        "        # Standardize continuous columns\n",
        "        self.mean = base_dummy[self.template.continuous].mean(axis=0)\n",
        "        self.std = base_dummy[self.template.continuous].std(axis=0)\n",
        "\n",
        "        base_dummy[self.template.continuous] = (base_dummy[self.template.continuous] - self.mean) / self.std\n",
        "\n",
        "        # Match columns from original dataset\n",
        "        self.x_data = []\n",
        "        for c in self.template.continuous:\n",
        "            if c in base_dummy.columns:\n",
        "                self.x_data.append(base_dummy[c].values.reshape(-1, 1))\n",
        "\n",
        "        # Get expected one-hot columns from template\n",
        "        template_dummy_cols = []\n",
        "        for col in self.template.train.columns:\n",
        "            if col not in self.template.continuous:\n",
        "                template_dummy_cols.append(col)\n",
        "\n",
        "        # Add one-hot columns in correct order\n",
        "        for col in template_dummy_cols:\n",
        "            if col in base_dummy.columns:\n",
        "                self.x_data.append(base_dummy[col].values.reshape(-1, 1))\n",
        "            else:\n",
        "                # Add zero column if missing\n",
        "                self.x_data.append(np.zeros((len(base_dummy), 1)))\n",
        "\n",
        "        # Concatenate all columns\n",
        "        self.x_data = np.hstack(self.x_data).astype(np.float32)\n",
        "        print(f\"Shadow dataset created with shape {self.x_data.shape}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.x_data[idx])\n",
        "\n",
        "def main():\n",
        "    # Get configuration\n",
        "    config = vars(get_args())\n",
        "    config[\"cuda\"] = torch.cuda.is_available()\n",
        "    device = torch.device('cuda:0') if config[\"cuda\"] else torch.device('cpu')\n",
        "\n",
        "    print(\"Configuration:\")\n",
        "    for k, v in config.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "    # Set random seed\n",
        "    set_random_seed(config[\"seed\"])\n",
        "    torch.manual_seed(config[\"seed\"])\n",
        "    if config[\"cuda\"]:\n",
        "        torch.cuda.manual_seed(config[\"seed\"])\n",
        "\n",
        "    # Import dataset module\n",
        "    import importlib\n",
        "    dataset_module = importlib.import_module(f'modules.{config[\"dataset\"]}_datasets')\n",
        "    TabularDataset = dataset_module.TabularDataset\n",
        "\n",
        "    # Get original dataset to use as template\n",
        "    orig_dataset = TabularDataset()\n",
        "\n",
        "    # Update config with dataset dimensions\n",
        "    OutputInfo_list = orig_dataset.OutputInfo_list\n",
        "    CRPS_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'CRPS'])\n",
        "    softmax_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'softmax'])\n",
        "    config[\"CRPS_dim\"] = CRPS_dim\n",
        "    config[\"softmax_dim\"] = softmax_dim\n",
        "\n",
        "    print(f\"Dataset dimensions - CRPS: {CRPS_dim}, Softmax: {softmax_dim}\")\n",
        "    print(f\"Expected input size: {CRPS_dim + softmax_dim}\")\n",
        "\n",
        "    # Load shadow datasets\n",
        "    K = 1  # Number of shadow models\n",
        "    shadow_datasets = []\n",
        "\n",
        "    for s in range(K):\n",
        "        shadow_path = f'./privacy/{config[\"dataset\"]}/train_{config[\"seed\"]}_synthetic{s}.csv'\n",
        "        if os.path.exists(shadow_path):\n",
        "            try:\n",
        "                # Create shadow dataset using original dataset as template\n",
        "                shadow_dataset = ShadowDataset(shadow_path, orig_dataset)\n",
        "                shadow_datasets.append(shadow_dataset)\n",
        "                print(f\"Loaded shadow dataset {s}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading shadow dataset {s}: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            print(f\"Shadow dataset {shadow_path} not found\")\n",
        "\n",
        "    if not shadow_datasets:\n",
        "        print(\"No shadow datasets found. Please generate shadow data first.\")\n",
        "        return\n",
        "\n",
        "    # Train shadow models\n",
        "    for k, shadow_dataset in enumerate(shadow_datasets):\n",
        "        print(f\"\\\\nTraining {k}th shadow model...\\\\n\")\n",
        "\n",
        "        # Initialize model\n",
        "        model = VAE(config, device).to(device)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "        model.train()\n",
        "\n",
        "        # Create dataloader\n",
        "        dataloader = DataLoader(shadow_dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
        "\n",
        "        # Training loop\n",
        "        for epoch in range(config[\"epochs\"]):\n",
        "            logs = train_VAE(OutputInfo_list, dataloader, model, config, optimizer, device)\n",
        "\n",
        "            print_input = f\"[epoch {epoch + 1:03d}]\"\n",
        "            print_input += ''.join([f\", {x}: {np.mean(y):.4f}\" for x, y in logs.items()])\n",
        "            print(print_input)\n",
        "\n",
        "        # Save shadow model\n",
        "        os.makedirs('./assets/shadow', exist_ok=True)\n",
        "        torch.save(model.state_dict(), f'./assets/shadow/shadow_DistVAE_{config[\"dataset\"]}_{k}.pth')\n",
        "        print(f\"Shadow model {k} saved to ./assets/shadow/shadow_DistVAE_{config['dataset']}_{k}.pth\")\n",
        "\n",
        "    print(\"\\\\nShadow model training complete!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"Created simplified shadow_main.py that properly handles the data format\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_dCiNKtSfRb",
        "outputId": "c63b6ad7-cbdc-49a9-b5aa-a04890d72646",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created simplified shadow_main.py that properly handles the data format\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHADOW ATTACK**"
      ],
      "metadata": {
        "id": "9RkkVVP7SvUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix shadow_attack.py to handle empty shadow test data\n",
        "with open('shadow_attack.py', 'w') as f:\n",
        "    f.write('''\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tqdm\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "\n",
        "from modules.simulation import set_random_seed\n",
        "from modules.model import VAE\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "\n",
        "# Configuration\n",
        "import argparse\n",
        "def get_args():\n",
        "    parser = argparse.ArgumentParser('parameters')\n",
        "\n",
        "    parser.add_argument('--dataset', type=str, default='adult',\n",
        "                        help='Dataset options: covtype, credit, loan, adult, cabs, kings')\n",
        "    parser.add_argument('--seed', type=int, default=1,\n",
        "                        help='random seed')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "# Custom Dataset class that matches format with original dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, df, original_dataset):\n",
        "        \"\"\"\n",
        "        Create a dataset with exact same column structure as the original\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.orig = original_dataset\n",
        "\n",
        "        # Create numeric versions of all features\n",
        "        df_processed = pd.DataFrame()\n",
        "\n",
        "        # Process continuous features\n",
        "        for col in self.orig.continuous:\n",
        "            if col in self.df.columns:\n",
        "                df_processed[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
        "\n",
        "        # Standardize\n",
        "        df_processed[self.orig.continuous] = (df_processed[self.orig.continuous] -\n",
        "                                             df_processed[self.orig.continuous].mean()) / df_processed[self.orig.continuous].std()\n",
        "\n",
        "        # Process discrete features\n",
        "        for i, dis in enumerate(self.orig.discrete):\n",
        "            if dis in self.df.columns:\n",
        "                # Convert to numeric using original mapping\n",
        "                if self.df[dis].dtype == 'object':\n",
        "                    mapping = self.orig.discrete_dicts[i]\n",
        "                    df_processed[dis] = self.df[dis].map(lambda x: mapping.get(x, 0) if x in mapping else 0)\n",
        "                else:\n",
        "                    df_processed[dis] = self.df[dis]\n",
        "\n",
        "                # One-hot encode\n",
        "                dummies = pd.get_dummies(df_processed[dis], prefix=dis)\n",
        "                for col in dummies.columns:\n",
        "                    df_processed[col] = dummies[col]\n",
        "\n",
        "        # Make sure we have all columns expected by the model\n",
        "        original_columns = self.orig.train.columns\n",
        "        for col in original_columns:\n",
        "            if col not in df_processed.columns:\n",
        "                df_processed[col] = 0\n",
        "\n",
        "        # Get data in the right order and format\n",
        "        self.data = df_processed[original_columns].to_numpy().astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.FloatTensor(self.data[idx])\n",
        "\n",
        "def main():\n",
        "    # Get configuration\n",
        "    args = get_args()\n",
        "    config = vars(args)\n",
        "\n",
        "    # Add required configuration parameters\n",
        "    config[\"latent_dim\"] = 2\n",
        "    config[\"step\"] = 0.1\n",
        "    config[\"threshold\"] = 1e-5\n",
        "    config[\"batch_size\"] = 256  # Add batch_size\n",
        "\n",
        "    # Basic setup\n",
        "    if not os.path.exists('./privacy/results'):\n",
        "        os.makedirs('./privacy/results')\n",
        "\n",
        "    config[\"cuda\"] = torch.cuda.is_available()\n",
        "    device = torch.device('cuda:0') if config[\"cuda\"] else torch.device('cpu')\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "    print(f\"Running privacy attack for dataset: {config['dataset']}\")\n",
        "\n",
        "    # Set random seed\n",
        "    set_random_seed(config[\"seed\"])\n",
        "    torch.manual_seed(config[\"seed\"])\n",
        "    if config[\"cuda\"]:\n",
        "        torch.cuda.manual_seed(config[\"seed\"])\n",
        "\n",
        "    # Import dataset module to get OutputInfo_list\n",
        "    import importlib\n",
        "    dataset_module = importlib.import_module(f'modules.{config[\"dataset\"]}_datasets')\n",
        "    TabularDataset = dataset_module.TabularDataset\n",
        "\n",
        "    # Get OutputInfo_list from original dataset\n",
        "    dataset = TabularDataset()\n",
        "    test_dataset = TabularDataset(train=False)\n",
        "\n",
        "    # Check if test dataset has data\n",
        "    has_test_data = len(test_dataset.x_data) > 0\n",
        "    if not has_test_data:\n",
        "        print(\"Warning: Test dataset is empty. Using a subset of training data as test set.\")\n",
        "\n",
        "    OutputInfo_list = dataset.OutputInfo_list\n",
        "    CRPS_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'CRPS'])\n",
        "    softmax_dim = sum([x.dim for x in OutputInfo_list if x.activation_fn == 'softmax'])\n",
        "    config[\"CRPS_dim\"] = CRPS_dim\n",
        "    config[\"softmax_dim\"] = softmax_dim\n",
        "\n",
        "    # Load shadow models\n",
        "    K = 1  # Number of shadow models\n",
        "    shadow_models = []\n",
        "    for k in range(K):\n",
        "        model_path = f'./assets/shadow/shadow_DistVAE_{config[\"dataset\"]}_{k}.pth'\n",
        "        if os.path.exists(model_path):\n",
        "            model = VAE(config, device).to(device)\n",
        "            model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "            model.eval()\n",
        "            shadow_models.append(model)\n",
        "            print(f\"Loaded shadow model {k}\")\n",
        "        else:\n",
        "            print(f\"Shadow model {model_path} not found\")\n",
        "\n",
        "    if not shadow_models:\n",
        "        print(\"No shadow models found. Please train shadow models first.\")\n",
        "        return\n",
        "\n",
        "    # Determine classification target\n",
        "    if config[\"dataset\"] == \"covtype\":\n",
        "        target = 'Cover_Type'\n",
        "    elif config[\"dataset\"] == \"credit\":\n",
        "        target = 'TARGET'\n",
        "    elif config[\"dataset\"] == \"loan\":\n",
        "        target = 'Personal Loan'\n",
        "    elif config[\"dataset\"] == \"adult\":\n",
        "        target = 'income'\n",
        "    elif config[\"dataset\"] == \"cabs\":\n",
        "        target = 'Gender'\n",
        "    elif config[\"dataset\"] == \"kings\":\n",
        "        target = 'condition'\n",
        "    else:\n",
        "        raise ValueError('Not supported dataset!')\n",
        "\n",
        "    # Load and process shadow data\n",
        "    shadow_data_train = []\n",
        "    shadow_targets_train = []\n",
        "    for k in range(K):\n",
        "        try:\n",
        "            # Load training data (in)\n",
        "            df = pd.read_csv(f'./privacy/{config[\"dataset\"]}/train_{config[\"seed\"]}_synthetic{k}.csv')\n",
        "\n",
        "            # Create properly formatted dataset\n",
        "            shadow_data_train.append(CustomDataset(df, dataset))\n",
        "\n",
        "            # Get targets\n",
        "            if target in df.columns:\n",
        "                if df[target].dtype == 'object':\n",
        "                    # For string targets\n",
        "                    target_idx = dataset.discrete.index(target)\n",
        "                    target_dict = dataset.discrete_dicts[target_idx]\n",
        "                    shadow_targets_train.append(df[target].map(target_dict).fillna(0).astype(int).values)\n",
        "                else:\n",
        "                    shadow_targets_train.append(df[target].astype(int).values)\n",
        "                print(f\"Loaded shadow training dataset {k}\")\n",
        "            else:\n",
        "                print(f\"Target column {target} not found in shadow dataset {k}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading shadow training dataset {k}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    shadow_data_test = []\n",
        "    shadow_targets_test = []\n",
        "    for k in range(K):\n",
        "        try:\n",
        "            # Load test data (out)\n",
        "            test_path = f'./privacy/{config[\"dataset\"]}/test_{config[\"seed\"]}_synthetic{k}.csv'\n",
        "            if os.path.exists(test_path):\n",
        "                df = pd.read_csv(test_path)\n",
        "\n",
        "                if len(df) > 0:\n",
        "                    # Create properly formatted dataset\n",
        "                    shadow_data_test.append(CustomDataset(df, dataset))\n",
        "\n",
        "                    # Get targets\n",
        "                    if target in df.columns:\n",
        "                        if df[target].dtype == 'object':\n",
        "                            # For string targets\n",
        "                            target_idx = dataset.discrete.index(target)\n",
        "                            target_dict = dataset.discrete_dicts[target_idx]\n",
        "                            shadow_targets_test.append(df[target].map(target_dict).fillna(0).astype(int).values)\n",
        "                        else:\n",
        "                            shadow_targets_test.append(df[target].astype(int).values)\n",
        "                        print(f\"Loaded shadow test dataset {k}\")\n",
        "                    else:\n",
        "                        print(f\"Target column {target} not found in shadow test dataset {k}\")\n",
        "                else:\n",
        "                    print(f\"Shadow test dataset {k} is empty\")\n",
        "            else:\n",
        "                print(f\"Shadow test dataset {test_path} not found\")\n",
        "\n",
        "            # If no test data, use a portion of train data as test\n",
        "            if not shadow_data_test:\n",
        "                print(\"No shadow test data available. Using a portion of shadow train data as test.\")\n",
        "                # Take the last 20% of the shadow training data as test\n",
        "                if k < len(shadow_data_train) and k < len(shadow_targets_train):\n",
        "                    train_size = len(shadow_data_train[k])\n",
        "                    split_point = int(0.8 * train_size)\n",
        "\n",
        "                    # Create a copy of the train dataset\n",
        "                    train_dataset = shadow_data_train[k]\n",
        "                    train_targets = shadow_targets_train[k]\n",
        "\n",
        "                    # Split into train and test\n",
        "                    # For targets, we can just split the numpy array\n",
        "                    train_targets_split = train_targets[:split_point]\n",
        "                    test_targets_split = train_targets[split_point:]\n",
        "                    shadow_targets_train[k] = train_targets_split\n",
        "                    shadow_targets_test.append(test_targets_split)\n",
        "\n",
        "                    # For data, we need to create a new Dataset that fetches from the correct indices\n",
        "                    class SubsetDataset(Dataset):\n",
        "                        def __init__(self, dataset, indices):\n",
        "                            self.dataset = dataset\n",
        "                            self.indices = indices\n",
        "\n",
        "                        def __len__(self):\n",
        "                            return len(self.indices)\n",
        "\n",
        "                        def __getitem__(self, idx):\n",
        "                            return self.dataset[self.indices[idx]]\n",
        "\n",
        "                    # Create train and test subset datasets\n",
        "                    train_indices = list(range(split_point))\n",
        "                    test_indices = list(range(split_point, train_size))\n",
        "\n",
        "                    shadow_data_train[k] = SubsetDataset(train_dataset, train_indices)\n",
        "                    shadow_data_test.append(SubsetDataset(train_dataset, test_indices))\n",
        "                    print(f\"Created test set from training data with {len(test_indices)} samples\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing shadow test dataset {k}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "    # Verify we have both train and test data\n",
        "    if not shadow_data_train or not shadow_data_test:\n",
        "        print(\"Error: Missing either shadow train or test datasets\")\n",
        "        return\n",
        "\n",
        "    # Extract latent representations from training data (in)\n",
        "    latents = []\n",
        "    for k in range(len(shadow_models)):\n",
        "        if k < len(shadow_data_train):\n",
        "            dataloader = DataLoader(shadow_data_train[k], batch_size=config[\"batch_size\"], shuffle=False)\n",
        "            zs = []\n",
        "            for x_batch in tqdm.tqdm(dataloader, desc=f\"Extracting latents from shadow train {k}\"):\n",
        "                if config[\"cuda\"]:\n",
        "                    x_batch = x_batch.cuda()\n",
        "                with torch.no_grad():\n",
        "                    mean, _ = shadow_models[k].get_posterior(x_batch)\n",
        "                zs.append(mean.cpu().numpy())\n",
        "            if zs:\n",
        "                zs = np.vstack(zs)\n",
        "                latents.append(zs)\n",
        "                print(f\"Extracted {len(zs)} latent vectors from shadow train {k}\")\n",
        "            else:\n",
        "                print(f\"No latent vectors extracted from shadow train {k}\")\n",
        "\n",
        "    # Extract latent representations from test data (out)\n",
        "    latents_test = []\n",
        "    for k in range(len(shadow_models)):\n",
        "        if k < len(shadow_data_test):\n",
        "            dataloader = DataLoader(shadow_data_test[k], batch_size=config[\"batch_size\"], shuffle=False)\n",
        "            zs = []\n",
        "            for x_batch in tqdm.tqdm(dataloader, desc=f\"Extracting latents from shadow test {k}\"):\n",
        "                if config[\"cuda\"]:\n",
        "                    x_batch = x_batch.cuda()\n",
        "                with torch.no_grad():\n",
        "                    mean, _ = shadow_models[k].get_posterior(x_batch)\n",
        "                zs.append(mean.cpu().numpy())\n",
        "            if zs:\n",
        "                zs = np.vstack(zs)\n",
        "                latents_test.append(zs)\n",
        "                print(f\"Extracted {len(zs)} latent vectors from shadow test {k}\")\n",
        "            else:\n",
        "                print(f\"No latent vectors extracted from shadow test {k}\")\n",
        "\n",
        "    # Verify we have both train and test latent representations\n",
        "    if not latents or not latents_test:\n",
        "        print(\"Error: Missing latent representations from either train or test data\")\n",
        "        # If we have train latents but no test latents, create test latents from train\n",
        "        if latents and not latents_test:\n",
        "            print(\"Creating test latents by splitting train latents\")\n",
        "            for k in range(len(latents)):\n",
        "                train_size = len(latents[k])\n",
        "                split_point = int(0.8 * train_size)\n",
        "                latents_test.append(latents[k][split_point:])\n",
        "                latents[k] = latents[k][:split_point]\n",
        "\n",
        "                # Also split the targets\n",
        "                if k < len(shadow_targets_train):\n",
        "                    shadow_targets_test.append(shadow_targets_train[k][split_point:])\n",
        "                    shadow_targets_train[k] = shadow_targets_train[k][:split_point]\n",
        "        else:\n",
        "            return\n",
        "\n",
        "    # Find unique classes\n",
        "    unique_classes = set()\n",
        "    for targets_list in shadow_targets_train:\n",
        "        unique_classes.update(np.unique(targets_list))\n",
        "    for targets_list in shadow_targets_test:\n",
        "        unique_classes.update(np.unique(targets_list))\n",
        "\n",
        "    target_num = len(unique_classes)\n",
        "    print(f\"Found {target_num} unique target classes\")\n",
        "\n",
        "    # Prepare attack training data\n",
        "    attack_training = {}\n",
        "    for t in unique_classes:\n",
        "        # In samples (from training)\n",
        "        in_samples = []\n",
        "        for k in range(len(latents)):\n",
        "            if k < len(shadow_targets_train):\n",
        "                class_indices = np.where(shadow_targets_train[k] == t)[0]\n",
        "                if len(class_indices) > 0:\n",
        "                    in_samples.append(latents[k][class_indices])\n",
        "\n",
        "        # Out samples (from testing)\n",
        "        out_samples = []\n",
        "        for k in range(len(latents_test)):\n",
        "            if k < len(shadow_targets_test):\n",
        "                class_indices = np.where(shadow_targets_test[k] == t)[0]\n",
        "                if len(class_indices) > 0:\n",
        "                    out_samples.append(latents_test[k][class_indices])\n",
        "\n",
        "        if in_samples and out_samples:\n",
        "            in_data = np.vstack(in_samples)\n",
        "            in_data = np.hstack([in_data, np.ones((len(in_data), 1))])  # Add in/out label (1 = in)\n",
        "\n",
        "            out_data = np.vstack(out_samples)\n",
        "            out_data = np.hstack([out_data, np.zeros((len(out_data), 1))])  # Add in/out label (0 = out)\n",
        "\n",
        "            attack_data = np.vstack([in_data, out_data])\n",
        "            attack_training[t] = attack_data\n",
        "\n",
        "            print(f\"Class {t}: {len(in_data)} in samples, {len(out_data)} out samples\")\n",
        "\n",
        "    # Train attack models\n",
        "    attackers = {}\n",
        "    for t in attack_training:\n",
        "        attack_data = attack_training[t]\n",
        "        clf = GradientBoostingClassifier(random_state=0).fit(\n",
        "            attack_data[:, :config[\"latent_dim\"]],\n",
        "            attack_data[:, -1])\n",
        "        attackers[t] = clf\n",
        "        print(f\"Trained attacker for class {t}\")\n",
        "\n",
        "    # Load the target model\n",
        "    model_path = f'./assets/DistVAE_{config[\"dataset\"]}.pth'\n",
        "    target_model = VAE(config, device).to(device)\n",
        "    target_model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    target_model.eval()\n",
        "\n",
        "    # Get ground-truth latent representations\n",
        "    dataloader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "    gt_latents = []\n",
        "    for x_batch in tqdm.tqdm(dataloader, desc=\"Extracting latents from real train\"):\n",
        "        if config[\"cuda\"]:\n",
        "            x_batch = x_batch.cuda()\n",
        "        with torch.no_grad():\n",
        "            mean, _ = target_model.get_posterior(x_batch)\n",
        "        gt_latents.append(mean.cpu().numpy())\n",
        "    gt_latents = np.vstack(gt_latents)\n",
        "\n",
        "    # Check if test data is available\n",
        "    if has_test_data and len(test_dataset.x_data) > 0:\n",
        "        dataloader_test = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
        "        gt_latents_test = []\n",
        "        for x_batch in tqdm.tqdm(dataloader_test, desc=\"Extracting latents from real test\"):\n",
        "            if config[\"cuda\"]:\n",
        "                x_batch = x_batch.cuda()\n",
        "            with torch.no_grad():\n",
        "                mean, _ = target_model.get_posterior(x_batch)\n",
        "            gt_latents_test.append(mean.cpu().numpy())\n",
        "        if gt_latents_test:\n",
        "            gt_latents_test = np.vstack(gt_latents_test)\n",
        "        else:\n",
        "            # If no test latents were extracted, split training latents\n",
        "            print(\"No test latents were extracted. Using portion of training latents as test.\")\n",
        "            train_size = len(gt_latents)\n",
        "            split_point = int(0.8 * train_size)\n",
        "            gt_latents_test = gt_latents[split_point:]\n",
        "            gt_latents = gt_latents[:split_point]\n",
        "    else:\n",
        "        # If no test data, use a portion of training data as \"test\"\n",
        "        train_size = len(gt_latents)\n",
        "        split_point = int(0.8 * train_size)\n",
        "        gt_latents_test = gt_latents[split_point:]\n",
        "        gt_latents = gt_latents[:split_point]\n",
        "        print(f\"Using {len(gt_latents_test)} samples from training as test set\")\n",
        "\n",
        "    # Get ground-truth targets\n",
        "    gt_targets = []\n",
        "    gt_targets_test = []\n",
        "\n",
        "    # Try to get target column from train_raw\n",
        "    if hasattr(dataset, 'train_raw') and target in dataset.train_raw.columns:\n",
        "        if dataset.train_raw[target].dtype == 'object':\n",
        "            target_idx = dataset.discrete.index(target)\n",
        "            gt_targets_all = np.array([dataset.discrete_dicts[target_idx].get(x, 0)\n",
        "                                 for x in dataset.train_raw[target].values])\n",
        "\n",
        "            # Split according to latent split if we used train data for test\n",
        "            if len(gt_latents) < len(dataset.train_raw):\n",
        "                split_point = len(gt_latents)\n",
        "                gt_targets = gt_targets_all[:split_point]\n",
        "                gt_targets_test = gt_targets_all[split_point:split_point+len(gt_latents_test)]\n",
        "            else:\n",
        "                gt_targets = gt_targets_all\n",
        "\n",
        "                if has_test_data and hasattr(test_dataset, 'test_raw') and target in test_dataset.test_raw.columns:\n",
        "                    gt_targets_test = np.array([dataset.discrete_dicts[target_idx].get(x, 0)\n",
        "                                             for x in test_dataset.test_raw[target].values])\n",
        "                else:\n",
        "                    # If we couldn't get test targets, log an error\n",
        "                    print(\"Could not get test targets. Privacy evaluation may not be accurate.\")\n",
        "                    # Try to create some targets for testing from training data\n",
        "                    train_size = len(gt_targets)\n",
        "                    split_point = int(0.8 * train_size)\n",
        "                    gt_targets_test = gt_targets[split_point:]\n",
        "                    gt_targets = gt_targets[:split_point]\n",
        "        else:\n",
        "            gt_targets_all = dataset.train_raw[target].values\n",
        "\n",
        "            # Split according to latent split if we used train data for test\n",
        "            if len(gt_latents) < len(dataset.train_raw):\n",
        "                split_point = len(gt_latents)\n",
        "                gt_targets = gt_targets_all[:split_point]\n",
        "                gt_targets_test = gt_targets_all[split_point:split_point+len(gt_latents_test)]\n",
        "            else:\n",
        "                gt_targets = gt_targets_all\n",
        "\n",
        "                if has_test_data and hasattr(test_dataset, 'test_raw') and target in test_dataset.test_raw.columns:\n",
        "                    gt_targets_test = test_dataset.test_raw[target].values\n",
        "                else:\n",
        "                    # If we couldn't get test targets, use a portion of train targets\n",
        "                    train_size = len(gt_targets)\n",
        "                    split_point = int(0.8 * train_size)\n",
        "                    gt_targets_test = gt_targets[split_point:]\n",
        "                    gt_targets = gt_targets[:split_point]\n",
        "\n",
        "    # If failed to get targets, try to derive from the one-hot columns\n",
        "    if len(gt_targets) == 0:\n",
        "        target_cols = [c for c in dataset.train.columns if c.startswith(f\"{target}_\")]\n",
        "        if target_cols:\n",
        "            try:\n",
        "                gt_targets_all = dataset.train[target_cols].idxmax(axis=1).str.replace(f\"{target}_\", \"\").astype(int).values\n",
        "\n",
        "                # Split according to latent split if we used train data for test\n",
        "                if len(gt_latents) < len(dataset.train):\n",
        "                    split_point = len(gt_latents)\n",
        "                    gt_targets = gt_targets_all[:split_point]\n",
        "                    gt_targets_test = gt_targets_all[split_point:split_point+len(gt_latents_test)]\n",
        "                else:\n",
        "                    gt_targets = gt_targets_all\n",
        "\n",
        "                    if has_test_data:\n",
        "                        gt_targets_test = test_dataset.test[target_cols].idxmax(axis=1).str.replace(f\"{target}_\", \"\").astype(int).values\n",
        "                    else:\n",
        "                        # Use portion of train targets as test\n",
        "                        train_size = len(gt_targets)\n",
        "                        split_point = int(0.8 * train_size)\n",
        "                        gt_targets_test = gt_targets[split_point:]\n",
        "                        gt_targets = gt_targets[:split_point]\n",
        "            except:\n",
        "                # Another attempt with different approach\n",
        "                gt_targets_all = np.argmax(dataset.train[target_cols].values, axis=1)\n",
        "\n",
        "                # Split according to latent split if we used train data for test\n",
        "                if len(gt_latents) < len(dataset.train):\n",
        "                    split_point = len(gt_latents)\n",
        "                    gt_targets = gt_targets_all[:split_point]\n",
        "                    gt_targets_test = gt_targets_all[split_point:split_point+len(gt_latents_test)]\n",
        "                else:\n",
        "                    gt_targets = gt_targets_all\n",
        "\n",
        "                    if has_test_data:\n",
        "                        gt_targets_test = np.argmax(test_dataset.test[target_cols].values, axis=1)\n",
        "                    else:\n",
        "                        # Use portion of train targets as test\n",
        "                        train_size = len(gt_targets)\n",
        "                        split_point = int(0.8 * train_size)\n",
        "                        gt_targets_test = gt_targets[split_point:]\n",
        "                        gt_targets = gt_targets[:split_point]\n",
        "\n",
        "    if len(gt_targets) == 0 or len(gt_targets_test) == 0:\n",
        "        print(f\"Could not extract target values for {target}\")\n",
        "        return\n",
        "\n",
        "    # Make sure targets match latents in length\n",
        "    if len(gt_targets) > len(gt_latents):\n",
        "        gt_targets = gt_targets[:len(gt_latents)]\n",
        "    elif len(gt_targets) < len(gt_latents):\n",
        "        gt_latents = gt_latents[:len(gt_targets)]\n",
        "\n",
        "    if len(gt_targets_test) > len(gt_latents_test):\n",
        "        gt_targets_test = gt_targets_test[:len(gt_latents_test)]\n",
        "    elif len(gt_targets_test) < len(gt_latents_test):\n",
        "        gt_latents_test = gt_latents_test[:len(gt_targets_test)]\n",
        "\n",
        "    print(f\"Extracted target values with {len(np.unique(gt_targets))} unique classes\")\n",
        "    print(f\"Train samples: {len(gt_latents)}, Test samples: {len(gt_latents_test)}\")\n",
        "\n",
        "    # Use same number of samples from both sets for balanced evaluation\n",
        "    min_samples = min(len(gt_latents), len(gt_latents_test))\n",
        "    gt_latents = gt_latents[:min_samples]\n",
        "    gt_targets = gt_targets[:min_samples]\n",
        "    gt_latents_test = gt_latents_test[:min_samples]\n",
        "    gt_targets_test = gt_targets_test[:min_samples]\n",
        "\n",
        "    # Perform membership inference attack\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "\n",
        "    # Test on real training data (should be \"in\")\n",
        "    for t in attackers:\n",
        "        class_indices = np.where(gt_targets == t)[0]\n",
        "        if len(class_indices) > 0:\n",
        "            latents_class = gt_latents[class_indices]\n",
        "            preds = attackers[t].predict(latents_class)\n",
        "            all_preds.extend(preds)\n",
        "            all_true.extend(np.ones_like(preds))  # These are \"in\" samples\n",
        "\n",
        "    # Test on real test data (should be \"out\")\n",
        "    for t in attackers:\n",
        "        class_indices = np.where(gt_targets_test == t)[0]\n",
        "        if len(class_indices) > 0:\n",
        "            latents_class = gt_latents_test[class_indices]\n",
        "            preds = attackers[t].predict(latents_class)\n",
        "            all_preds.extend(preds)\n",
        "            all_true.extend(np.zeros_like(preds))  # These are \"out\" samples\n",
        "\n",
        "    # Calculate attack performance metrics\n",
        "    if all_preds and all_true:\n",
        "        all_preds = np.array(all_preds)\n",
        "        all_true = np.array(all_true)\n",
        "\n",
        "        acc = accuracy_score(all_true, all_preds)\n",
        "        f1 = f1_score(all_true, all_preds)\n",
        "        try:\n",
        "            auc = roc_auc_score(all_true, all_preds)\n",
        "        except:\n",
        "            auc = 0.5  # Default for random guessing\n",
        "\n",
        "        print('\\\\nMembership Inference Attack Results:')\n",
        "        print(f'Accuracy: {acc:.3f}')\n",
        "        print(f'F1: {f1:.3f}')\n",
        "        print(f'AUC: {auc:.3f}')\n",
        "\n",
        "        # Save results\n",
        "        privacy_results = {\n",
        "            'dataset': config['dataset'],\n",
        "            'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "            'MI_accuracy': acc,\n",
        "            'MI_f1': f1,\n",
        "            'MI_auc': auc\n",
        "        }\n",
        "\n",
        "        os.makedirs('./privacy/results', exist_ok=True)\n",
        "        pd.DataFrame([privacy_results]).to_csv(\n",
        "            f'./privacy/results/{config[\"dataset\"]}_privacy_evaluation.csv', index=False)\n",
        "\n",
        "        print(f\"\\\\nPrivacy evaluation complete! Results saved to ./privacy/results/{config['dataset']}_privacy_evaluation.csv\")\n",
        "    else:\n",
        "        print(\"No predictions made. Attack could not be completed.\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "''')\n",
        "\n",
        "print(\"Further improved shadow_attack.py to handle empty shadow test latents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2T1URmrWSsd1",
        "outputId": "3dcdc771-88b8-4591-b715-37ec0ac9840e",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Further improved shadow_attack.py to handle empty shadow test latents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GENERATE THE SHADOW DATASETS**"
      ],
      "metadata": {
        "id": "7i3tfREpS7cb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create necessary directories\n",
        "!mkdir -p ./privacy\n",
        "!mkdir -p ./assets/shadow\n",
        "\n",
        "# Generate shadow datasets\n",
        "!python shadow_data.py --dataset adult"
      ],
      "metadata": {
        "id": "LTPnv6dpS4Sq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ca3e004-06ea-454c-bef3-9394bb67c2a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Generating shadow data for dataset: adult\n",
            "/content/modules/adult_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/adult_datasets.py:87: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:88: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_adult.pth\n",
            "Generating shadow data...\n",
            "Generating shadow train and test datasets...:   0% 0/1 [00:00<?, ?it/s]Shadow training data saved to ./privacy/adult/train_1_synthetic0.csv\n",
            "Shadow test data saved to ./privacy/adult/test_1_synthetic0.csv\n",
            "Generating shadow train and test datasets...: 100% 1/1 [00:01<00:00,  1.01s/it]\n",
            "Shadow data generation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SHADOW MODELS**"
      ],
      "metadata": {
        "id": "idvSTrApT6FS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_main.py --dataset adult"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ReYo3V6VT5c8",
        "outputId": "b4a62277-f5d9-4cec-f1cd-780f00c51e90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: adult\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 5\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "/content/modules/adult_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Dataset dimensions - CRPS: 5, Softmax: 100\n",
            "Expected input size: 105\n",
            "Shadow dataset created with shape (40000, 105)\n",
            "Loaded shadow dataset 0\n",
            "\n",
            "Training 0th shadow model...\n",
            "\n",
            "inner loop: 100% 157/157 [00:03<00:00, 49.62it/s]\n",
            "[epoch 001], loss: 12.6243, quantile: 12.5355, KL: 0.1776, activated: 0.0000\n",
            "inner loop: 100% 157/157 [00:02<00:00, 58.08it/s]\n",
            "[epoch 002], loss: 9.2607, quantile: 8.8916, KL: 0.7382, activated: 0.0000\n",
            "inner loop: 100% 157/157 [00:02<00:00, 56.25it/s]\n",
            "[epoch 003], loss: 8.3813, quantile: 7.6073, KL: 1.5481, activated: 0.0000\n",
            "inner loop: 100% 157/157 [00:02<00:00, 58.17it/s]\n",
            "[epoch 004], loss: 7.7583, quantile: 6.6471, KL: 2.2223, activated: 0.8917\n",
            "inner loop: 100% 157/157 [00:02<00:00, 58.54it/s]\n",
            "[epoch 005], loss: 7.4891, quantile: 6.2427, KL: 2.4928, activated: 1.1656\n",
            "Shadow model 0 saved to ./assets/shadow/shadow_DistVAE_adult_0.pth\n",
            "\n",
            "Shadow model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_attack.py --dataset adult"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "BrBLp2rcVjsm",
        "outputId": "e01c9ac7-17da-4dad-ab24-2b3a85d6fbef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Running privacy attack for dataset: adult\n",
            "/content/modules/adult_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/adult_datasets.py:87: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/adult_datasets.py:88: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loaded shadow model 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "Loaded shadow training dataset 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "/content/shadow_attack.py:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df_processed[col] = 0\n",
            "Loaded shadow test dataset 0\n",
            "Extracting latents from shadow train 0: 100% 157/157 [00:00<00:00, 347.31it/s]\n",
            "Extracting latents from shadow test 0: 100% 21/21 [00:00<00:00, 417.64it/s]\n",
            "Found 2 unique target classes\n",
            "Class 0: 32036 in samples, 4199 out samples\n",
            "Class 1: 7964 in samples, 1023 out samples\n",
            "Trained attacker for class 0\n",
            "Trained attacker for class 1\n",
            "Extracting latents from real train: 100% 157/157 [00:00<00:00, 542.64it/s]\n",
            "Extracting latents from real test: 100% 21/21 [00:00<00:00, 564.57it/s]\n",
            "Extracted target values with 2 unique classes\n",
            "\n",
            "Membership Inference Attack Results:\n",
            "Accuracy: 0.500\n",
            "F1: 0.667\n",
            "AUC: 0.500\n",
            "\n",
            "Privacy evaluation complete! Results saved to ./privacy/results/adult_privacy_evaluation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_data.py --dataset covtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P57TG2MBeg2v",
        "outputId": "8c6ff3ad-1322-4ccf-f185-733f5078562f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Generating shadow data for dataset: covtype\n",
            "/content/modules/covtype_datasets.py:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/covtype_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:85: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_covtype.pth\n",
            "Generating shadow data...\n",
            "Generating shadow train and test datasets...:   0% 0/1 [00:00<?, ?it/s]Shadow training data saved to ./privacy/covtype/train_1_synthetic0.csv\n",
            "Shadow test data saved to ./privacy/covtype/test_1_synthetic0.csv\n",
            "Generating shadow train and test datasets...: 100% 1/1 [00:00<00:00,  1.13it/s]\n",
            "Shadow data generation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_main.py --dataset covtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXh7fQTUgE5z",
        "outputId": "39692c1a-020b-4826-fc96-1c20cf475d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: covtype\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 5\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "/content/modules/covtype_datasets.py:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Dataset dimensions - CRPS: 10, Softmax: 7\n",
            "Expected input size: 17\n",
            "Shadow dataset created with shape (45000, 17)\n",
            "Loaded shadow dataset 0\n",
            "\n",
            "Training 0th shadow model...\n",
            "\n",
            "inner loop: 100% 176/176 [00:04<00:00, 35.52it/s]\n",
            "[epoch 001], loss: 4.4321, quantile: 4.4205, KL: 0.0233, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 40.27it/s]\n",
            "[epoch 002], loss: 3.9379, quantile: 3.9351, KL: 0.0057, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 39.96it/s]\n",
            "[epoch 003], loss: 3.9027, quantile: 3.8656, KL: 0.0741, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 40.20it/s]\n",
            "[epoch 004], loss: 3.7125, quantile: 3.4178, KL: 0.5893, activated: 0.0000\n",
            "inner loop: 100% 176/176 [00:04<00:00, 40.26it/s]\n",
            "[epoch 005], loss: 3.5940, quantile: 3.1744, KL: 0.8393, activated: 0.0000\n",
            "Shadow model 0 saved to ./assets/shadow/shadow_DistVAE_covtype_0.pth\n",
            "\n",
            "Shadow model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_attack.py --dataset covtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfiQn2KTY6lg",
        "outputId": "b73b4ccf-b53d-4023-b946-5ca257543c9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Running privacy attack for dataset: covtype\n",
            "/content/modules/covtype_datasets.py:69: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:70: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/covtype_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/covtype_datasets.py:85: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loaded shadow model 0\n",
            "Loaded shadow training dataset 0\n",
            "Loaded shadow test dataset 0\n",
            "Extracting latents from shadow train 0: 100% 176/176 [00:00<00:00, 383.93it/s]\n",
            "Extracting latents from shadow test 0: 100% 20/20 [00:00<00:00, 384.19it/s]\n",
            "Found 7 unique target classes\n",
            "Class 0: 16264 in samples, 1804 out samples\n",
            "Class 1: 23119 in samples, 2572 out samples\n",
            "Class 2: 2077 in samples, 261 out samples\n",
            "Class 3: 73 in samples, 7 out samples\n",
            "Class 4: 684 in samples, 72 out samples\n",
            "Class 5: 1250 in samples, 132 out samples\n",
            "Class 6: 1533 in samples, 152 out samples\n",
            "Trained attacker for class 0\n",
            "Trained attacker for class 1\n",
            "Trained attacker for class 2\n",
            "Trained attacker for class 3\n",
            "Trained attacker for class 4\n",
            "Trained attacker for class 5\n",
            "Trained attacker for class 6\n",
            "Extracting latents from real train: 100% 176/176 [00:00<00:00, 490.97it/s]\n",
            "Extracting latents from real test: 100% 20/20 [00:00<00:00, 577.10it/s]\n",
            "Extracted target values with 7 unique classes\n",
            "\n",
            "Membership Inference Attack Results:\n",
            "Accuracy: 0.496\n",
            "F1: 0.382\n",
            "AUC: 0.496\n",
            "\n",
            "Privacy evaluation complete! Results saved to ./privacy/results/covtype_privacy_evaluation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_data.py --dataset credit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyaJZTXZgS3O",
        "outputId": "1f641acf-12b9-4b72-aff6-f47d39be546b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Generating shadow data for dataset: credit\n",
            "Loading model from ./assets/DistVAE_credit.pth\n",
            "Generating shadow data...\n",
            "Generating shadow train and test datasets...:   0% 0/1 [00:00<?, ?it/s]Shadow training data saved to ./privacy/credit/train_1_synthetic0.csv\n",
            "Shadow test data saved to ./privacy/credit/test_1_synthetic0.csv\n",
            "Generating shadow train and test datasets...: 100% 1/1 [00:00<00:00,  1.44it/s]\n",
            "Shadow data generation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_main.py --dataset credit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QkDmysLgVe-",
        "outputId": "85044e65-574e-4ecd-ee74-c0990c79a997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: credit\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 5\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "Dataset dimensions - CRPS: 10, Softmax: 37\n",
            "Expected input size: 47\n",
            "Shadow dataset created with shape (16953, 47)\n",
            "Loaded shadow dataset 0\n",
            "\n",
            "Training 0th shadow model...\n",
            "\n",
            "inner loop: 100% 67/67 [00:02<00:00, 29.84it/s]\n",
            "[epoch 001], loss: 11.9140, quantile: 11.8831, KL: 0.0617, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 37.18it/s]\n",
            "[epoch 002], loss: 8.2069, quantile: 8.1859, KL: 0.0421, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 38.16it/s]\n",
            "[epoch 003], loss: 7.8724, quantile: 7.8587, KL: 0.0274, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 36.67it/s]\n",
            "[epoch 004], loss: 7.7954, quantile: 7.7685, KL: 0.0537, activated: 0.0000\n",
            "inner loop: 100% 67/67 [00:01<00:00, 37.20it/s]\n",
            "[epoch 005], loss: 7.7480, quantile: 7.6984, KL: 0.0993, activated: 0.0000\n",
            "Shadow model 0 saved to ./assets/shadow/shadow_DistVAE_credit_0.pth\n",
            "\n",
            "Shadow model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_attack.py --dataset credit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rguJa1rZEvk",
        "outputId": "048a5f6d-7b1e-4681-aa92-ef9d28b4297c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Running privacy attack for dataset: credit\n",
            "Warning: Test dataset is empty. Using a subset of training data as test set.\n",
            "Loaded shadow model 0\n",
            "Loaded shadow training dataset 0\n",
            "Shadow test dataset 0 is empty\n",
            "No shadow test data available. Using a portion of shadow train data as test.\n",
            "Created test set from training data with 3391 samples\n",
            "Extracting latents from shadow train 0: 100% 53/53 [00:00<00:00, 310.62it/s]\n",
            "Extracted 13562 latent vectors from shadow train 0\n",
            "Extracting latents from shadow test 0: 100% 14/14 [00:00<00:00, 567.25it/s]\n",
            "Extracted 3391 latent vectors from shadow test 0\n",
            "Found 2 unique target classes\n",
            "Class 0: 12561 in samples, 3163 out samples\n",
            "Class 1: 1001 in samples, 228 out samples\n",
            "Trained attacker for class 0\n",
            "Trained attacker for class 1\n",
            "Extracting latents from real train: 100% 67/67 [00:00<00:00, 589.56it/s]\n",
            "Using 3391 samples from training as test set\n",
            "Extracted target values with 2 unique classes\n",
            "Train samples: 13562, Test samples: 3391\n",
            "\n",
            "Membership Inference Attack Results:\n",
            "Accuracy: 0.486\n",
            "F1: 0.451\n",
            "AUC: 0.486\n",
            "\n",
            "Privacy evaluation complete! Results saved to ./privacy/results/credit_privacy_evaluation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_data.py --dataset loan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-yQmcfXg8U5",
        "outputId": "38524e9b-f76a-44be-b401-a1017735f3a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Generating shadow data for dataset: loan\n",
            "/content/modules/loan_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/loan_datasets.py:87: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:88: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_loan.pth\n",
            "Generating shadow data...\n",
            "Generating shadow train and test datasets...:   0% 0/1 [00:00<?, ?it/s]Shadow training data saved to ./privacy/loan/train_1_synthetic0.csv\n",
            "Shadow test data saved to ./privacy/loan/test_1_synthetic0.csv\n",
            "Generating shadow train and test datasets...: 100% 1/1 [00:00<00:00,  4.22it/s]\n",
            "Shadow data generation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_main.py --dataset loan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEnQXqrShA_m",
        "outputId": "d2a4e3fa-bc72-4984-9043-712ab3f17f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: loan\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 5\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "/content/modules/loan_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Dataset dimensions - CRPS: 5, Softmax: 14\n",
            "Expected input size: 19\n",
            "Shadow dataset created with shape (4000, 19)\n",
            "Loaded shadow dataset 0\n",
            "\n",
            "Training 0th shadow model...\n",
            "\n",
            "inner loop: 100% 16/16 [00:00<00:00, 22.63it/s]\n",
            "[epoch 001], loss: 7.0327, quantile: 7.0089, KL: 0.0477, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 61.09it/s]\n",
            "[epoch 002], loss: 6.0717, quantile: 6.0572, KL: 0.0290, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 64.63it/s]\n",
            "[epoch 003], loss: 5.4376, quantile: 5.4217, KL: 0.0317, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 60.13it/s]\n",
            "[epoch 004], loss: 5.1674, quantile: 5.1545, KL: 0.0258, activated: 0.0000\n",
            "inner loop: 100% 16/16 [00:00<00:00, 60.97it/s]\n",
            "[epoch 005], loss: 5.0379, quantile: 5.0307, KL: 0.0145, activated: 0.0000\n",
            "Shadow model 0 saved to ./assets/shadow/shadow_DistVAE_loan_0.pth\n",
            "\n",
            "Shadow model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_attack.py --dataset loan"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIoJ85RWZbHX",
        "outputId": "d9869355-ab59-430f-8a83-3deff90bbfb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Running privacy attack for dataset: loan\n",
            "/content/modules/loan_datasets.py:72: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:73: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/loan_datasets.py:87: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/loan_datasets.py:88: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loaded shadow model 0\n",
            "Loaded shadow training dataset 0\n",
            "Loaded shadow test dataset 0\n",
            "Extracting latents from shadow train 0: 100% 16/16 [00:00<00:00, 214.06it/s]\n",
            "Extracting latents from shadow test 0: 100% 4/4 [00:00<00:00, 419.76it/s]\n",
            "Found 2 unique target classes\n",
            "Class 0: 3613 in samples, 912 out samples\n",
            "Class 1: 387 in samples, 88 out samples\n",
            "Trained attacker for class 0\n",
            "Trained attacker for class 1\n",
            "Extracting latents from real train: 100% 16/16 [00:00<00:00, 379.32it/s]\n",
            "Extracting latents from real test: 100% 4/4 [00:00<00:00, 387.50it/s]\n",
            "Extracted target values with 2 unique classes\n",
            "\n",
            "Membership Inference Attack Results:\n",
            "Accuracy: 0.502\n",
            "F1: 0.657\n",
            "AUC: 0.502\n",
            "\n",
            "Privacy evaluation complete! Results saved to ./privacy/results/loan_privacy_evaluation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_data.py --dataset cabs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvZbLzYxhHjb",
        "outputId": "eecddc1c-87c8-4323-d2ab-d29102669548"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Generating shadow data for dataset: cabs\n",
            "/content/modules/cabs_datasets.py:78: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:79: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/cabs_datasets.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:96: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_cabs.pth\n",
            "Generating shadow data...\n",
            "Generating shadow train and test datasets...:   0% 0/1 [00:00<?, ?it/s]Shadow training data saved to ./privacy/cabs/train_1_synthetic0.csv\n",
            "Shadow test data saved to ./privacy/cabs/test_1_synthetic0.csv\n",
            "Generating shadow train and test datasets...: 100% 1/1 [00:00<00:00,  1.32it/s]\n",
            "Shadow data generation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_main.py --dataset cabs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0gTn00khMd5",
        "outputId": "b759ad13-aeaf-4762-90ab-0b8d14fc749d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: cabs\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 5\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "/content/modules/cabs_datasets.py:78: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:79: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Dataset dimensions - CRPS: 6, Softmax: 44\n",
            "Expected input size: 50\n",
            "Shadow dataset created with shape (22329, 50)\n",
            "Loaded shadow dataset 0\n",
            "\n",
            "Training 0th shadow model...\n",
            "\n",
            "inner loop: 100% 88/88 [00:02<00:00, 43.83it/s]\n",
            "[epoch 001], loss: 10.9109, quantile: 10.8703, KL: 0.0812, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 55.50it/s]\n",
            "[epoch 002], loss: 8.8549, quantile: 8.8319, KL: 0.0460, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 54.91it/s]\n",
            "[epoch 003], loss: 8.7091, quantile: 8.6919, KL: 0.0345, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 50.43it/s]\n",
            "[epoch 004], loss: 8.6457, quantile: 8.6024, KL: 0.0867, activated: 0.0000\n",
            "inner loop: 100% 88/88 [00:01<00:00, 51.97it/s]\n",
            "[epoch 005], loss: 8.4762, quantile: 8.2489, KL: 0.4546, activated: 0.0000\n",
            "Shadow model 0 saved to ./assets/shadow/shadow_DistVAE_cabs_0.pth\n",
            "\n",
            "Shadow model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_attack.py --dataset cabs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqdc1VYVZqLe",
        "outputId": "4a9be11d-8cbd-4f50-f0bc-7241608e4767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Running privacy attack for dataset: cabs\n",
            "/content/modules/cabs_datasets.py:78: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:79: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/cabs_datasets.py:95: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/cabs_datasets.py:96: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loaded shadow model 0\n",
            "Loaded shadow training dataset 0\n",
            "Loaded shadow test dataset 0\n",
            "Extracting latents from shadow train 0: 100% 88/88 [00:00<00:00, 440.70it/s]\n",
            "Extracting latents from shadow test 0: 100% 22/22 [00:00<00:00, 566.55it/s]\n",
            "Found 2 unique target classes\n",
            "Class 0: 6680 in samples, 1636 out samples\n",
            "Class 1: 15649 in samples, 3947 out samples\n",
            "Trained attacker for class 0\n",
            "Trained attacker for class 1\n",
            "Extracting latents from real train: 100% 88/88 [00:00<00:00, 372.97it/s]\n",
            "Extracting latents from real test: 100% 22/22 [00:00<00:00, 373.69it/s]\n",
            "Extracted target values with 2 unique classes\n",
            "\n",
            "Membership Inference Attack Results:\n",
            "Accuracy: 0.503\n",
            "F1: 0.649\n",
            "AUC: 0.503\n",
            "\n",
            "Privacy evaluation complete! Results saved to ./privacy/results/cabs_privacy_evaluation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_data.py --dataset kings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QaFg9SXthfxb",
        "outputId": "a203f6ee-1d60-42f0-acff-5612694bdc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Generating shadow data for dataset: kings\n",
            "/content/modules/kings_datasets.py:83: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/kings_datasets.py:98: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:99: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loading model from ./assets/DistVAE_kings.pth\n",
            "Generating shadow data...\n",
            "Generating shadow train and test datasets...:   0% 0/1 [00:00<?, ?it/s]Shadow training data saved to ./privacy/kings/train_1_synthetic0.csv\n",
            "Shadow test data saved to ./privacy/kings/test_1_synthetic0.csv\n",
            "Generating shadow train and test datasets...: 100% 1/1 [00:00<00:00,  1.39it/s]\n",
            "Shadow data generation complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_main.py --dataset kings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYGA7rqnhk3s",
        "outputId": "f8cb835a-6249-417b-9406-1794488dfc43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Configuration:\n",
            "  seed: 1\n",
            "  dataset: kings\n",
            "  latent_dim: 2\n",
            "  step: 0.1\n",
            "  epochs: 5\n",
            "  batch_size: 256\n",
            "  lr: 0.001\n",
            "  threshold: 1e-05\n",
            "  beta: 0.5\n",
            "  cuda: True\n",
            "/content/modules/kings_datasets.py:83: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Dataset dimensions - CRPS: 11, Softmax: 73\n",
            "Expected input size: 84\n",
            "Shadow dataset created with shape (20000, 84)\n",
            "Loaded shadow dataset 0\n",
            "\n",
            "Training 0th shadow model...\n",
            "\n",
            "inner loop: 100% 79/79 [00:02<00:00, 28.28it/s]\n",
            "[epoch 001], loss: 13.8386, quantile: 13.7752, KL: 0.1270, activated: 0.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 33.71it/s]\n",
            "[epoch 002], loss: 9.5540, quantile: 9.4556, KL: 0.1967, activated: 0.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 33.90it/s]\n",
            "[epoch 003], loss: 8.5681, quantile: 8.0908, KL: 0.9545, activated: 0.0000\n",
            "inner loop: 100% 79/79 [00:02<00:00, 33.92it/s]\n",
            "[epoch 004], loss: 7.9885, quantile: 7.3737, KL: 1.2296, activated: 0.2025\n",
            "inner loop: 100% 79/79 [00:02<00:00, 34.41it/s]\n",
            "[epoch 005], loss: 7.7289, quantile: 7.0021, KL: 1.4535, activated: 0.9873\n",
            "Shadow model 0 saved to ./assets/shadow/shadow_DistVAE_kings_0.pth\n",
            "\n",
            "Shadow model training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python shadow_attack.py --dataset kings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEj9nR68Z28g",
        "outputId": "30e82082-ab43-4cbf-d4aa-4cc6ce63dbca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda:0\n",
            "Running privacy attack for dataset: kings\n",
            "/content/modules/kings_datasets.py:83: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:84: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "/content/modules/kings_datasets.py:98: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] = df[self.continuous] - self.mean\n",
            "/content/modules/kings_datasets.py:99: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df[self.continuous] /= self.std\n",
            "Loaded shadow model 0\n",
            "Loaded shadow training dataset 0\n",
            "Loaded shadow test dataset 0\n",
            "Extracting latents from shadow train 0: 100% 79/79 [00:00<00:00, 342.22it/s]\n",
            "Extracting latents from shadow test 0: 100% 7/7 [00:00<00:00, 393.43it/s]\n",
            "Found 4 unique target classes\n",
            "Class 1: 17 in samples, 4 out samples\n",
            "Class 2: 13763 in samples, 1085 out samples\n",
            "Class 3: 5280 in samples, 437 out samples\n",
            "Class 4: 940 in samples, 87 out samples\n",
            "Trained attacker for class 1\n",
            "Trained attacker for class 2\n",
            "Trained attacker for class 3\n",
            "Trained attacker for class 4\n",
            "Extracting latents from real train: 100% 79/79 [00:00<00:00, 381.03it/s]\n",
            "Extracting latents from real test: 100% 7/7 [00:00<00:00, 390.32it/s]\n",
            "Extracted target values with 5 unique classes\n",
            "\n",
            "Membership Inference Attack Results:\n",
            "Accuracy: 0.496\n",
            "F1: 0.650\n",
            "AUC: 0.496\n",
            "\n",
            "Privacy evaluation complete! Results saved to ./privacy/results/kings_privacy_evaluation.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a summary of all evaluation results\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Find all evaluation files\n",
        "result_files = glob.glob('./assets/results/*_evaluation.csv')\n",
        "\n",
        "# Read and combine results\n",
        "dfs = []\n",
        "for file in result_files:\n",
        "    df = pd.read_csv(file)\n",
        "    dfs.append(df)\n",
        "\n",
        "if dfs:\n",
        "    all_results = pd.concat(dfs)\n",
        "\n",
        "    # Create a summary table\n",
        "    summary = all_results[['dataset', 'K-S_continuous', '1-WD_continuous',\n",
        "                          'DCR_R&S', 'AD_F1_K10']]\n",
        "\n",
        "    # Save summary\n",
        "    summary.to_csv('./assets/results/summary.csv', index=False)\n",
        "\n",
        "    # Display the summary\n",
        "    print(\"Summary of evaluation results:\")\n",
        "    print(summary)\n",
        "else:\n",
        "    print(\"No evaluation results found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PaVGZdbSW6v9",
        "outputId": "02b8e6d5-b1e3-4b93-a8ea-0390642b5441"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary of evaluation results:\n",
            "   dataset  K-S_continuous  1-WD_continuous   DCR_R&S  AD_F1_K10\n",
            "0  covtype        0.043640         0.064336  0.810164   0.000000\n",
            "0    kings        0.145727         0.122728  0.573692   0.127746\n",
            "0     loan        0.128200         0.178420  0.314732   0.392122\n",
            "0     cabs        0.055981         0.078423  0.401437   0.158693\n",
            "0    adult        0.142070         0.151013  0.066024   0.146588\n",
            "0   credit        0.128874         0.127680  0.841083   0.291246\n"
          ]
        }
      ]
    }
  ]
}